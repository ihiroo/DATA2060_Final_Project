{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CART**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART is not the same as ID3.\n",
    "\n",
    "CART typically performs binary splits. For continuous features, it finds the optimal threshold to split the data, creating binary splits. It also handle categorical features natively. \n",
    "\n",
    "CART uses Gini impurity for classification problems. For pruning, it includes a cost-complexity pruning method that allows for post-pruning to improve generalization and avoid overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Representation** NOTE: MODIFY THIS TO ALIGN WITH THE ACTUAL CODE!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: entropy or training error can also be used as the splitting criterion, but it's more common to use Gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{CART}(S, A)$$\n",
    "$\\text{Inputs: training set }S\\text{, feature subset }A\\subseteq [d]$<br>\n",
    "$\\textbf{if }\\text{all examples in }S\\text{ are labeled by }1\\text{, return a leaf }1$<br>\n",
    "$\\textbf{if }\\text{all examples in }S\\text{ are labeled by }0\\text{, return a leaf }0$<br>\n",
    "$\\textbf{if }A=\\emptyset\\text{, return a leaf whose value }=\\text{ majority of labels in }S$<br>\n",
    "$\\textbf{else}\\text{:}$<br>\n",
    "$\\quad \\textbf{foreach}\\text{ feature }i\\text{ in }A\\text{:}$<br>\n",
    "$\\quad\\quad\\textbf{if }x_i\\text{ is continuous: }$<br>\n",
    "$\\quad\\quad\\quad\\text{Generate a sequence of thresholds }\\theta \\text{ based on the sorted values of }x_i$<br>\n",
    "$\\quad\\quad\\quad\\textbf{foreach}\\text{ threshold }\\theta\\text{ in the sequence:}$<br>\n",
    "$\\quad\\quad\\quad\\quad S_1 = \\{(\\textbf{x},y)\\in S:x_i\\le\\theta\\}$<br>\n",
    "$\\quad\\quad\\quad\\quad S_2 = \\{(\\textbf{x},y)\\in S:x_i>\\theta\\}$<br>\n",
    "$\\quad\\quad\\quad\\quad \\text{Gini}(S,i)=\\frac{|S_1|}{|S|}\\cdot\\text{Gini}(S_1)+\\frac{|S_2|}{|S|}\\cdot\\text{Gini}(S_2)$<br>\n",
    "$\\quad\\quad\\textbf{if }x_i\\text{ is categorical: }$<br>\n",
    "$\\quad\\quad\\quad\\text{Let categories }= \\text{ unique values of }x_i$<br>\n",
    "$\\quad\\quad\\quad\\textbf{foreach}\\text{ binary split }(a,b)\\text{ of categories:}$<br>\n",
    "$\\quad\\quad\\quad\\quad S_1 = \\{(\\textbf{x},y)\\in S:x_i\\in a\\}$<br>\n",
    "$\\quad\\quad\\quad\\quad S_2 = \\{(\\textbf{x},y)\\in S:x_i\\in b\\}$<br>\n",
    "$\\quad\\quad\\quad\\quad \\text{Gini}(S,i)=\\frac{|S_1|}{|S|}\\cdot\\text{Gini}(S_1)+\\frac{|S_2|}{|S|}\\cdot\\text{Gini}(S_2)$<br>\n",
    "$\\quad \\text{Let } j = \\text{argmin}_{i\\in A}\\text{Gini}(S,i)$<br>\n",
    "$\\quad \\textbf{if }j \\text{ is continuous:}$<br>\n",
    "$\\quad\\quad S_1 = \\{(\\textbf{x},y)\\in S:x_j\\le\\theta\\}$<br>\n",
    "$\\quad\\quad S_2 = \\{(\\textbf{x},y)\\in S:x_j>\\theta\\}$<br>\n",
    "$\\quad \\textbf{if }j \\text{ is categorical:}$<br>\n",
    "$\\quad\\quad S_1 = \\{(\\textbf{x},y)\\in S:x_j\\in a\\}$<br>\n",
    "$\\quad\\quad S_2 = \\{(\\textbf{x},y)\\in S:x_j\\in b\\}$<br>\n",
    "$\\quad\\text{Let }T_1\\text{ be the tree returned by CART}(S_1,A\\backslash\\{j\\})$<br>\n",
    "$\\quad\\text{Let }T_2\\text{ be the tree returned by CART}(S_2,A\\backslash\\{j\\})$<br>\n",
    "$\\quad\\text{Return the tree}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loss**\n",
    "$$\\text{Gini}(S)=1-\\sum^K_{k=1}p_k^2$$\n",
    "We split two subsets namely $S_1$ and $S_2$, and we have the Gini impurity of $S_1$ and $S_2$ as:\n",
    "$$\\text{Gini}(S_1)=1-\\sum^K_{k=1}p_k^2$$\n",
    "$$\\text{Gini}(S_2)=1-\\sum^K_{k=1}p_k^2$$\n",
    "Where:\n",
    "- $p_k$ is the proportion of samples in the subset that belong to class $k$.\n",
    "- $K$ is the number of unique labels in the dataset.\n",
    "\n",
    "In a binary classification problem, $K=2$ and we have only $p_1$ and $p_2$. Then:\n",
    "$$Gini(S)=1-(p_1^2+p_2^2)$$\n",
    "Letting $a=p_1$, we have:\n",
    "$$\\text{Gini}(S)=1-(a^2+(1-a)^2)=2a(1-a)$$\n",
    "After splitting, the weighted Gini impurity is calculated as:\n",
    "$$\\text{Gini}(S,i)=\\frac{|S_1|}{|S|}\\cdot\\text{Gini}(S_1)+\\frac{|S_2|}{|S|}\\cdot\\text{Gini}(S_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimizer**\n",
    "\n",
    "We will use pruning to avoid overfitting. Pruning in CART is done using cost-complexity pruning, where a penalty term is added.\n",
    "\n",
    "The cost-complexity function for a tree $T$ is:\n",
    "$$R_\\alpha(T)=R(T)+\\alpha\\cdot|T|$$\n",
    "Where:\n",
    "\n",
    "- $R(T)$ is the impurity or misclassification cost of the tree on $S$.\n",
    "- $|T|$ is the number of leaves in the tree.\n",
    "- $\\alpha > 0$ is a regularization parameter.\n",
    "\n",
    "We can use metrics like Gini impurity, entropy, or training error to calculate $R(T)$. For any given tree, $R(T)$ is the sum of the misclassification costs of all leaves in the tree. For each leaf $t$ in $T$, the misclassification cost $R(t)$ is defined as:\n",
    "$$R(t)=\\frac{1}{N_t}\\sum_{i\\in t}\\ell(y_i,\\hat y_{i,t})$$\n",
    "Where:\n",
    "-  $N_t$ is the number of samples in leaf $t$\n",
    "- $\\ell(y_i,\\hat y_{i,t})$ is the loss for one sample, where $y_i$ is the true label and $\\hat y_{i,t}$ is the predicted label for leaf $t$.\n",
    "\n",
    "For simplicity, we will use the 0-1 loss as the loss function here.\n",
    "$$\n",
    "\\ell(y_i, \\hat{y}_t) = \\begin{cases} \n",
    "1, & \\text{if } y_i \\neq \\hat{y}_{i,t} \\\\ \n",
    "0, & \\text{if } y_i = \\hat{y}_{i,t} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then, $R(T)$ is defined as:\n",
    "$$R(T)=\\sum_{t\\in\\text{leaves}(T)}R(t)$$\n",
    "Where $\\text{leaves}(T)$ is the set of all leaves in $T$.\n",
    "\n",
    "We will minimize $R_\\alpha(T)$ to find the one that best balances between complexity and loss and use cross-validation to select the best $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding**\n",
    "### EDA\n",
    "- No missing values in the dataset, and this is a balanced dataset.\n",
    "- `sex` and `exange` are binary.\n",
    "    - `exange`: 0 means no exercise-induced angina and 1 means exercise-induced angina.\n",
    "- `cp`, `fbs`, `restecg`, `slop`, `ca` and `thal` are ordinal. But there is no need to use the ordinal encoder since they are represented with numbers.\n",
    "    - `fbs` is binary, 0 means normal and 1 links to diabetes.\n",
    "    - `restecg`: 0 is the best (normal) and 2 is the worst (linked to severe cardiovascular issues).\n",
    "    - `cp`: hard to define the order, need to pay attention to this variable.\n",
    "    - `slope`: 0 is the best and 2 is the worst.\n",
    "    - `ca`: 0 is the best and 4 is the worst.\n",
    "    - `thal`: 1 is the best and 3 is the worst.\n",
    "- **Note: later use correlation matrix...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart.csv\")\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor\n",
    "cat_ftrs = [\"sex\", \"exang\"]\n",
    "num_ftrs = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"cp\", \"fbs\", \"restecg\", \"slope\", \"ca\", \"thal\"]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", categorical_transformer, cat_ftrs),\n",
    "    (\"num\", numerical_transformer, num_ftrs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML pipeline\n",
    "def MLpipe_kfold(X, y, random_states, preprocessor, ML_algo, param_grid, n_splits=5):\n",
    "    test_scores = []\n",
    "    best_models = []\n",
    "    for i, random_state in enumerate(random_states):\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        pipe = make_pipeline(preprocessor, ML_algo)\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid, cv=kf, n_jobs=-1, return_train_score=True, \n",
    "                            verbose=True, scoring=\"accuracy\")\n",
    "        grid.fit(X_other, y_other)\n",
    "        results = pd.DataFrame(grid.cv_results_)\n",
    "        best_models.append(grid)\n",
    "        y_test_pred = best_models[-1].predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_scores.append(test_accuracy)\n",
    "    return test_scores, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 160 candidates, totalling 1600 fits\n",
      "Average Testing Accuracy: 0.819672131147541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihiro/miniconda3/envs/data2060/lib/python3.12/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "random_states = [2060]\n",
    "ML_algo = DecisionTreeClassifier(random_state=2060, criterion=\"gini\")\n",
    "param_grid = {\n",
    "    \"decisiontreeclassifier__max_depth\": [None, 3, 5, 10, 20],\n",
    "    \"decisiontreeclassifier__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"decisiontreeclassifier__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"decisiontreeclassifier__max_leaf_nodes\": [5, 10]\n",
    "}\n",
    "test_scores, best_models = MLpipe_kfold(X, y, random_states, preprocessor, ML_algo, param_grid, n_splits=10)\n",
    "print(\"Average Testing Accuracy:\", np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "data.head() # 303 rows x 13 features, variable target is the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_size=0.4, random_state=2060):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    n_samples = data.shape[0]\n",
    "    indices = np.random.permutation(n_samples) # shuffling\n",
    "    test_size = int(n_samples * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    train_data = data[train_indices]\n",
    "    test_data = data[test_indices]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a node in the decision tree. Each node can be one of the following:\n",
    "    - Decision node: splits the data.\n",
    "    - Leaf node: predicts the label.\n",
    "\n",
    "    Parameters:\n",
    "    - left: none for leaf nodes.\n",
    "    - right: none for leaf nodes.\n",
    "    - label: none for decision nodes.\n",
    "    - feature: none for leaf nodes.\n",
    "    - threshold: none for leaf nodes or categorical splits.\n",
    "    \"\"\"\n",
    "    def __init__(self, left=None, right=None, label=None, feature=None, threshold=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Check if a node is a leaf node.\n",
    "        \"\"\"\n",
    "        return self.label is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "    def __init__(self, max_depth=10, min_samples_split=10):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - max_depth: int\n",
    "            The maximum depth of the tree.\n",
    "        - min_samples_split: int\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        - tree: Node\n",
    "            The node of the decision tree.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit the decision tree to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.ndarray\n",
    "            The training dataset where the last column is the target variable.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(data, depth=0)\n",
    "\n",
    "    def _build_tree(self, data, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the tree.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.ndarray\n",
    "            The training dataset at the current node, where the last column is the target variable.\n",
    "        - depth: int\n",
    "            The current depth of the tree.\n",
    "        \n",
    "        Returns:\n",
    "        - Node:\n",
    "            The node of the decision tree.\n",
    "        \"\"\"\n",
    "        labels = data[:, -1]\n",
    "        # Stopping conditions\n",
    "        # All labels are the same\n",
    "        if len(np.unique(labels)) == 1:\n",
    "            return Node(label=labels[0])\n",
    "        # Max depth or minimum split size is reached\n",
    "        # TODO: check if we need to add the = case?\n",
    "        if depth > self.max_depth or len(data) < self.min_samples_split:\n",
    "            major = np.bincount(labels.astype(int)).argmax()\n",
    "            return Node(label=major) # return the majority class\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._find_best_split(data)\n",
    "        # No valid split\n",
    "        if not best_split:\n",
    "            major = np.bincount(labels.astype(int)).argmax()\n",
    "            return Node(label=major) # return the majority class\n",
    "        # Remove the splitted categorical feature\n",
    "        if best_split[\"type\"] == \"categorical\":\n",
    "            remaining_data_left = np.delete(best_split[\"left\"], best_split[\"feature\"], axis=1)\n",
    "            remaining_data_right = np.delete(best_split[\"right\"], best_split[\"feature\"], axis=1)\n",
    "        else:\n",
    "            remaining_data_left = best_split[\"left\"]\n",
    "            remaining_data_right = best_split[\"right\"]\n",
    "        # Recursion\n",
    "        left_tree = self._build_tree(remaining_data_left, depth + 1)\n",
    "        right_tree = self._build_tree(remaining_data_right, depth + 1)\n",
    "        return Node(\n",
    "            left=left_tree,\n",
    "            right=right_tree,\n",
    "            feature=best_split[\"feature\"],\n",
    "            threshold=best_split[\"threshold\"]\n",
    "        )\n",
    "        \n",
    "    def _find_best_split(self, data):\n",
    "        \"\"\"\n",
    "        Find the best split for the current data.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.ndarray\n",
    "            The training dataset at the current node.\n",
    "        \n",
    "        Returns:\n",
    "        - A dictionary containing the best split information.\n",
    "        \"\"\"\n",
    "        best_gini = float(\"inf\") # use positive infinity since we will minimize it\n",
    "        best_split = None\n",
    "        n_features = data.shape[1] - 1 # last column is the target\n",
    "        for feature in range(n_features):\n",
    "            unique_values = np.unique(data[:, feature])\n",
    "            sorted_values = np.sort(unique_values)\n",
    "            if len(unique_values) > 2:\n",
    "                thresholds = (sorted_values[1:] + sorted_values[:-1]) / 2 # midpoints\n",
    "                for threshold in thresholds:\n",
    "                    left, right = self._split_continuous(data, feature, threshold)\n",
    "                    if len(left) == 0 or len(right) == 0:\n",
    "                        continue\n",
    "                    gini = self._gini_for_split(data, left, right)\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_split = {\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"continuous\"\n",
    "                        }\n",
    "            else:\n",
    "                left, right = self._split_categorical(data, feature)\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "                gini = self._gini_for_split(data, left, right)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {\n",
    "                        \"feature\": feature,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"left\": left,\n",
    "                        \"right\": right,\n",
    "                        \"type\": \"categorical\"\n",
    "                    }\n",
    "        return best_split\n",
    "    \n",
    "    # TODO: add a line to handle cases when len(data) = 0 in Gini?\n",
    "\n",
    "    def _gini_for_node(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity for a node.\n",
    "        \"\"\"\n",
    "        labels = data[:, -1] # the last column\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        probs = counts / len(data) # two probabilities of being in two classes\n",
    "        gini = 1 - np.sum(probs ** 2)\n",
    "        return gini\n",
    "\n",
    "    def _gini_for_split(self, data, left, right):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity for a split.\n",
    "        \"\"\"\n",
    "        total_size = len(data)\n",
    "        left_size = len(left)\n",
    "        right_size = len(right)\n",
    "        gini_left = gini_for_node(left)\n",
    "        gini_right = gini_for_node(right)\n",
    "        gini = (left_size / total_size) * gini_left + (right_size / total_size) * gini_right\n",
    "        return gini\n",
    "\n",
    "    def _split_continuous(self, data, feature_index, threshold):\n",
    "        \"\"\"\n",
    "        Split the data based on a continuous feature.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.ndarray\n",
    "            Dataset to split.\n",
    "        - feature_index: int\n",
    "            Index of the feature to split.\n",
    "        \n",
    "        Returns:\n",
    "        - Two subsets of the data.\n",
    "        \"\"\"\n",
    "        left = data[data[:, feature_index] <= threshold]\n",
    "        right = data[data[:, feature_index] > threshold]\n",
    "        return left, right\n",
    "\n",
    "    def _split_categorical(self, data, feature_index):\n",
    "        \"\"\"\n",
    "        Split the data based on a categorical feature.\n",
    "\n",
    "        Parameters:\n",
    "        - data: np.ndarray\n",
    "            Dataset to split.\n",
    "        - feature_index: int\n",
    "            Index of the feature to split.\n",
    "\n",
    "        Returns:\n",
    "        - Two subsets of the data.\n",
    "        \"\"\"\n",
    "        values = np.unique(data[:, feature_index])\n",
    "        threshold = np.mean(values)\n",
    "        left = data[data[:, feature_index] <= threshold]\n",
    "        right = data[data[:, feature_index] > threshold]\n",
    "        return left, right\n",
    "\n",
    "    def _predict_row(self, node, row):\n",
    "        \"\"\"\n",
    "        Predict the label for a single row.\n",
    "        \"\"\"\n",
    "        if node.is_leaf():\n",
    "            return node.label\n",
    "        # Categorical split\n",
    "        if node.threshold is None:\n",
    "            return self._predict_row(node.left, row) if row[node.feature] == 0 else self._predict_row(node.right, row)\n",
    "        else:  \n",
    "            return self._predict_row(node.left, row) if row[node.feature] <= node.threshold else self._predict_row(node.right, row)\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"\n",
    "        Predict the labels for the test data.\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_row(self.tree, row) for row in test_data])\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the loss for the test data.\n",
    "        \"\"\"\n",
    "        preds = self.predict(data[:, :-1])\n",
    "        true_labels = data[:, -1]\n",
    "        return np.sum(preds != true_labels) / len(true_labels)\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy for the test data.\n",
    "        \"\"\"\n",
    "        return 1 - self.loss(data)\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Visualize the decision tree.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            print(\"Empty tree.\")\n",
    "        else:\n",
    "            print(\"--- START PRINT TREE ---\")\n",
    "            self._visualize_tree(self.tree)\n",
    "            print(\"--- END PRINT TREE ---\")\n",
    "\n",
    "    def _visualize_tree(self, node, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively visualize the decision tree.\n",
    "        \"\"\"\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf():\n",
    "            print(f\"{indent}Predict -> {node.label}\")\n",
    "        else:\n",
    "            if node.threshold is None:\n",
    "                print(f\"{indent}split attribute = {node.feature}; categorical\")\n",
    "            else:\n",
    "                print(f\"{indent}split attribute = {node.feature}; threshold = {node.threshold:.3f}\")\n",
    "            print(f\"{indent}Left:\")\n",
    "            self._visualize_tree(node.left, depth + 1)\n",
    "            print(f\"{indent}Right:\")\n",
    "            self._visualize_tree(node.right, depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- START PRINT TREE ---\n",
      "split attribute = 0; threshold = 25.000\n",
      "Left:\n",
      "  Predict -> 0\n",
      "Right:\n",
      "  split attribute = 0; threshold = 45.000\n",
      "  Left:\n",
      "    Predict -> 1\n",
      "  Right:\n",
      "    Predict -> 0\n",
      "--- END PRINT TREE ---\n"
     ]
    }
   ],
   "source": [
    "# Tests for Gini calculations and splitting\n",
    "data = np.array([\n",
    "    [10, 0, 0],\n",
    "    [20, 1, 0],\n",
    "    [30, 0, 1],\n",
    "    [40, 1, 1],\n",
    "    [50, 0, 0]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the data\n",
    "data = np.loadtxt(\"heart.csv\", delimiter=\",\", skiprows=1)\n",
    "train_data, test_data = train_test_split(data, test_size=0.4, random_state=2060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.934\n",
      "Testing accuracy: 0.793\n",
      "--- START PRINT TREE ---\n",
      "split attribute = 11; threshold = 0.500\n",
      "Left:\n",
      "  split attribute = 12; threshold = 2.500\n",
      "  Left:\n",
      "    split attribute = 9; threshold = 2.700\n",
      "    Left:\n",
      "      split attribute = 7; threshold = 92.500\n",
      "      Left:\n",
      "        Predict -> 0.0\n",
      "      Right:\n",
      "        split attribute = 3; threshold = 158.000\n",
      "        Left:\n",
      "          split attribute = 9; threshold = 1.700\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 1\n",
      "        Right:\n",
      "          Predict -> 1\n",
      "    Right:\n",
      "      Predict -> 0\n",
      "  Right:\n",
      "    split attribute = 7; threshold = 143.500\n",
      "    Left:\n",
      "      split attribute = 9; threshold = 0.250\n",
      "      Left:\n",
      "        Predict -> 1.0\n",
      "      Right:\n",
      "        Predict -> 0.0\n",
      "    Right:\n",
      "      split attribute = 2; threshold = 0.500\n",
      "      Left:\n",
      "        Predict -> 0\n",
      "      Right:\n",
      "        split attribute = 0; threshold = 39.000\n",
      "        Left:\n",
      "          Predict -> 0.0\n",
      "        Right:\n",
      "          split attribute = 0; threshold = 63.500\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0.0\n",
      "Right:\n",
      "  split attribute = 2; threshold = 0.500\n",
      "  Left:\n",
      "    split attribute = 7; threshold = 108.500\n",
      "    Left:\n",
      "      Predict -> 0\n",
      "    Right:\n",
      "      Predict -> 0.0\n",
      "  Right:\n",
      "    split attribute = 10; threshold = 1.500\n",
      "    Left:\n",
      "      split attribute = 7; threshold = 145.500\n",
      "      Left:\n",
      "        Predict -> 0.0\n",
      "      Right:\n",
      "        Predict -> 0\n",
      "    Right:\n",
      "      split attribute = 3; threshold = 153.000\n",
      "      Left:\n",
      "        split attribute = 9; threshold = 2.550\n",
      "        Left:\n",
      "          split attribute = 4; threshold = 318.500\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0\n",
      "        Right:\n",
      "          Predict -> 0.0\n",
      "      Right:\n",
      "        Predict -> 0.0\n",
      "--- END PRINT TREE ---\n"
     ]
    }
   ],
   "source": [
    "model = CART(max_depth=10, min_samples_split=10)\n",
    "model.fit(train_data)\n",
    "train_accuracy = model.accuracy(train_data)\n",
    "test_accuracy = model.accuracy(test_data)\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Testing accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "model.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
