{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Representation\n",
    "#### Add algo pic from PPT (Use Alg_rep.png)\n",
    "The CART algorithm predicts a single outcome for binary classification by recursively partitioning the dataset using decision rules based on the input features. We require 4 inputs for the algorithm , including the training dataset $S$, feature subset $A$, and two values for the hyperparameters. When building the tree, if none of the stopping conditions are met, we first compute the Gini impurity for the current dataset, denoted as $\\text{Gini}(S)$. Then for each feature $\\mathbf{x}_i$ within the current feature subset $A$, we sorted the unique values of each feature $\\mathbf{x}_i$ and generate a sequence of midpoints based on the sorted values. For each midpoint, or threshold $\\theta$, the current dataset is split into two subsets, $S_1$ and $S_2$. Subset $S_1$ contains data points where $x_i \\leq \\theta$ , and subset $S_2$ contains data points where $x_i> \\theta$, with $x_i$ representing a value within the feature $ \\mathbf{x}_i$. Next, we calculate the Gini impurity for both subsets, $S_1$ and $S_2$, as well as the weighted Gini impurity, denoted as $\\text{Gini}(S_1,S_2,S)$. We measure the gain as the difference between the node Gini impurity $\\text{Gini}(S)$ and the weighted Gini impurity and choose the feature $\\mathbf{x}_j$ and threshold combination that will maximize this gain. Based on the chosen feature $\\mathbf{x}_j$ and threshold $\\theta$, we split the current dataset into two subset, $S_1$ and $S_2$, where subset $S_1$ contains data points where $x_i \\leq \\theta$ , and subset $S_2$ contains data points where $x_i> \\theta$. Using the strategy, we will recursively build the tree until one of the stopping conditions is met. Since in the Heart Disease dataset, all features are encoded as numeric values, this algorithm is enough for handling 13 numeric features. However, if the ordinal and categorical features are not encoded yet, this algorithm will not work and further modifications to the algorithm are required.\n",
    "\n",
    "We used 5 stopping conditions to terminate the recursion while building the tree. First, if all samples in the current subset $S$ are labeled as 1, a leaf node labeled 1 will be returned. Second, if all samples are labeled as 0, a leaf node labeled 0 will be returned. These two conditions ensure that the recursion stops when the node becomes pure, meaning all samples in the subset belong to the same class, and further splitting is unnecessary. Third, if the feature subset $A$ is empty, the recursion stops, and a leaf node will be returned with its value set to the majority label of the current subset $S$. This condition acts as a safeguard to prevent further splits when no features are left to evaluate, ensuring that the tree construction terminates gracefully even if the dataset cannot be split further in a meaningful way. Fourth, if the maximum tree depth is reached, the recursion terminates, and a leaf node will be returned with its value equal to the majority class label in $S$. The maximum tree depth is determined by the hyperparameter `max_depth`. Finally, if the size of the current subset $|S|$ is smaller than a predefined minimum number of samples (`min_samples_split`) required to split, a leaf node will be returned, with its value set to the majority class label in $S$. In the last three conditions, assigning the majority class label ensures that the algorithm no only provides a reasonable prediction even when further splitting is restricted by depth constraints, but also works for multi-class classification.\n",
    "\n",
    "# 1.4 Loss\n",
    "\n",
    "CART has 3 ways to calculate the loss, including the Gini impurity, entropy and misclassification error. This project uses Gini impurity since it's the default criterion in Scikit-learn's `DecisionTreeClassifier`. We will calculate the loss recursively when building up the tree. The parent Gini impurity, or the node Gini impurity is calculated as $$\\text{Gini}(S)=1-\\sum^K_{k=1}p_k^2\\text{,}$$ where $K$ is the number of unique classes in the entire dataset, and $p_k$ is the proportion of samples in the subset that belong to class $k$. In a binary classification problem, $$\\text{Gini}(S)=1-\\sum_{k=1}^2[P(y=k|S)]^2=1-(p_1^2+p_2^2)\\text{,}$$ \n",
    "\n",
    "The weighted Gini impurity is calculated as $$\\text{Gini}(S_1,S_2,S)=\\frac{|S_1|}{|S|}\\cdot\\text{Gini}(S_1)+\\frac{|S_2|}{|S|}\\cdot\\text{Gini}(S_2)\\text{,}$$ where $\\text{Gini}(S_1)$ and $\\text{Gini}(S_2)$ are the Gini impurity of the two splitted subsets $S_1$ and $S_2$, and we use $|\\cdot|$ to measure the cardinality, which is the number of samples within one dataset.\n",
    "\n",
    "Then, the gain from split is measured as the difference between the parent Gini impurity and the weighted Gini impurity. We then have\n",
    "$$\\text{Gain}(S_1,S_2,S)=\\text{Gini}(S)-\\text{Gini}(S_1,S_2,S)\\text{.}$$\n",
    "\n",
    "# 1.5 Optimizer: Pruning\n",
    "\n",
    "Pruning is used to optimize the DecisionTree classifier. In Scikit-learn, the DecisionTree classifier uses a parameter called cost-complexity pruning (ccp_alpha) to control the trade-off between the complexity of the tree and the loss. The reason why we need this is that pruning helps improve the generalization by cutting off unnecessary branches, thus preventing overfitting. During the pruning process, the algorithm will compute a cost-complexity measure for all the subtrees and add a penalty equal to ccp_alpha times the number of leaves in the subtrees. \n",
    "\n",
    "The pruning process starts from top to bottom, which means that it goes from the root and recursively selects each subtree and their children subtrees to perform this optimization. As shown in the graph below[6]. In the end, it should simplify the tree and reduce overfitting. The global cost function is written as $R_\\alpha(T) = R(T) + \\alpha * |T|$, T is the subtree, R(T) is the loss of the tree, and $|T|$ is the number of terminal nodes. The local pruning rule states that if $\\alpha > \\frac{R(t)-R(T_t)}{|T_t|-1}$, we can change this equation to this: prune $T_t$ if $(|T_t|-1) * \\alpha > R(t) - R(T_t)$ We iterate through different values of $\\alpha$ to find the optimal value that best improves the performance of the classifier. \n",
    "\n",
    "We also use cross-validation, specifically 5-fold cross-validation, for evaluating model performance by splitting the training data into 5 subsets. Initially, we split the entire dataset into 80% train and 20% test, then for the training dataset, we apply the 5-fold cross-validation to ensure robustness. In each iteration of the cross-validation process, it chooses the i-th fold as validation and the rest as training. The model is trained and validated iteratively across all folds to achieve reliable performance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun fact\n",
    "\n",
    "Our team name is UC Providence because we all graduated from the University of California, though from different campuses. Yixun graduated from UC Santa Barbara, David graduated from UC Irvine, and Liang graduated from UC San Diego. Interestingly, We didn't know we all went to the UC schools until our first meeting, and that's how we came up with the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from graphviz import Digraph\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    Constructor for the Node class\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, label=None, feature=None, threshold=None, parent_gini=None, node_gini=None, num_samples=None, class_counts=None):\n",
    "        self.left = left # to a left node\n",
    "        self.right = right # to a right node\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.parent_gini = parent_gini\n",
    "        self.node_gini = node_gini\n",
    "        self.num_samples = num_samples # data points in this node\n",
    "        self.class_counts = class_counts\n",
    "\n",
    "    def is_leaf(self):\n",
    "        '''\n",
    "        check if the node is a leaf node\n",
    "        '''\n",
    "        return self.label is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "    '''\n",
    "    Decision Tree classifier by UC Providence\n",
    "    '''\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, ccp_alpha=0.01, random_state=0):\n",
    "        if max_depth is None:\n",
    "            self.max_depth = 20\n",
    "        else:\n",
    "            self.max_depth = max_depth # set the max depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "        self.ccp_alpha = ccp_alpha # for pruning\n",
    "        self.random_state = random_state\n",
    "        if random_state is not None: # make random state deterministic\n",
    "            np.random.seed(random_state)\n",
    "            random.seed(random_state)\n",
    "        \n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        build the tree based on the data\n",
    "        '''\n",
    "        self.tree = self._build_tree(data, depth=0)\n",
    "    \n",
    "    def prune(self, ccp_alpha=0):\n",
    "        '''\n",
    "        prune the tree based on the ccp_alpha\n",
    "        '''\n",
    "        self._prune_tree(self.tree, ccp_alpha)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        '''\n",
    "        Helper function to predict the data\n",
    "        '''\n",
    "        X = data[:, :-1] # get the features\n",
    "        return np.array([self._predict_row(self.tree, row) for row in X])\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the loss\n",
    "        '''\n",
    "        preds = self.predict(data)\n",
    "        true_labels = data[:, -1] # last column is the label\n",
    "        return np.sum(preds != true_labels) / len(true_labels)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the accuracy\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "    \n",
    "    def _gini_for_node(self, data):\n",
    "        '''\n",
    "        Get the gini index for a node\n",
    "        params data: the data in the node\n",
    "        return: the gini index\n",
    "        '''\n",
    "        labels = data[:, -1] # get the last column which is the label\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probs = counts / len(labels)\n",
    "        parent_gini = 1 - np.sum(probs ** 2) # calculate the gini index\n",
    "        return parent_gini\n",
    "\n",
    "    def _gini_for_split(self, data, left, right):\n",
    "        '''\n",
    "        Get the gini index for a split\n",
    "        params data: the data in the node\n",
    "        params left: the left split\n",
    "        params right: the right split\n",
    "        return: the gini index\n",
    "        '''\n",
    "        # calc the total size\n",
    "        total_size = len(data)\n",
    "        left_size = len(left)\n",
    "        right_size = len(right)\n",
    "        # calc the gini index for the left and right\n",
    "        left_gini = self._gini_for_node(left)\n",
    "        right_gini = self._gini_for_node(right)\n",
    "        # calc the weighted gini index\n",
    "        weighted_gini = (left_size / total_size) * left_gini + (right_size / total_size) * right_gini\n",
    "        return weighted_gini\n",
    "\n",
    "    def _split(self, data, feature_index, threshold):\n",
    "        '''\n",
    "        Split the data based on the feature and threshold\n",
    "        params data: the data\n",
    "        params feature_index: the feature to split on\n",
    "        params threshold: the threshold to split on\n",
    "        return: the left and right split\n",
    "        '''\n",
    "        left = data[data[:, feature_index] <= threshold]\n",
    "        right = data[data[:, feature_index] > threshold]\n",
    "        return left, right\n",
    "\n",
    "    def _find_best_split(self, data):\n",
    "        '''\n",
    "        Find the best split for the data, traverse through each column and each average value of the values in the column to find the best split.\n",
    "        params data: the dataset\n",
    "        return: the best gain and the best split\n",
    "        '''\n",
    "        best_gain = float(\"-inf\")\n",
    "        best_split = None\n",
    "        best_split_list = [] # for ties\n",
    "        parent_gini = self._gini_for_node(data) # calc the gini index for the parent node\n",
    "        n_features = data.shape[1] - 1\n",
    "        for feature in range(n_features): # traverse through each feature\n",
    "            unique_values = np.unique(data[:, feature])\n",
    "            sorted_values = np.sort(unique_values)\n",
    "            thresholds = (sorted_values[1:] + sorted_values[:-1]) / 2 # get the average of the values\n",
    "\n",
    "            if len(thresholds) > 2:\n",
    "                # Continuous or ordinal features\n",
    "                for threshold in thresholds:\n",
    "                    left, right = self._split(data, feature, threshold)\n",
    "                    if len(left) == 0 or len(right) == 0:\n",
    "                        continue # skip if the split is empty\n",
    "                    weighted_gini = self._gini_for_split(data, left, right) # calc the weighted gini index\n",
    "                    gain = parent_gini - weighted_gini\n",
    "                    if gain > best_gain: # if the gain is better than the best gain\n",
    "                        best_gain = gain\n",
    "                        best_split_list = [{\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"continuous\"\n",
    "                        }]\n",
    "                    elif np.isclose(gain, best_gain):    \n",
    "                        best_gain = gain\n",
    "                        best_split_list.append({\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"continuous\"\n",
    "                        }) # if tied, then append to the list\n",
    "            else:\n",
    "                # Only one threshold for binary features\n",
    "                for threshold in thresholds:\n",
    "                    left, right = self._split(data, feature, threshold)\n",
    "                    if len(left) == 0 or len(right) == 0:\n",
    "                        continue\n",
    "                    weighted_gini = self._gini_for_split(data, left, right)\n",
    "                    gain = parent_gini - weighted_gini\n",
    "                    if gain > best_gain:    \n",
    "                        best_gain = gain\n",
    "                        # same for binary features but with different type\n",
    "                        best_split_list = [{\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"binary\"\n",
    "                        }]\n",
    "                    elif np.isclose(gain, best_gain):    \n",
    "                        best_gain = gain\n",
    "                        best_split_list.append({\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"binary\"\n",
    "                        })\n",
    "        if len(best_split_list) > 1:\n",
    "            # if the best split list has more than one best split then we sort it by feature\n",
    "            #print(\"Multiple best splits found\")\n",
    "            #x = sorted(best_split_list, key=lambda x: x[\"feature\"])\n",
    "            # for i in x:\n",
    "            #     print(i[\"feature\"], i[\"threshold\"], i[\"gini_for_split\"], i[\"parent_gini\"], i[\"gain\"])\n",
    "            best_split = sorted(best_split_list, key=lambda x: x[\"feature\"])[-1]\n",
    "        else: # else we just get the first best split\n",
    "            best_split = best_split_list[0]\n",
    "        return best_gain, best_split\n",
    "\n",
    "    def _majority_class(self, data):\n",
    "        '''\n",
    "        Get the majority class in the data\n",
    "        '''\n",
    "        labels = data[:, -1] # get the labels\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        return unique_labels[np.argmax(counts)]\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        '''\n",
    "        Build the tree recursively\n",
    "        params data: the data\n",
    "        params depth: the depth of the tree\n",
    "        return: the node and its attributes\n",
    "        '''\n",
    "        labels = data[:, -1] # label is the last column\n",
    "        num_samples = len(labels)\n",
    "        parent_gini = self._gini_for_node(data)\n",
    "\n",
    "        # Stopping conditions\n",
    "        # Having a pure node\n",
    "        if len(np.unique(labels)) == 1:\n",
    "            return Node(label=labels[0], parent_gini=parent_gini, num_samples=num_samples)\n",
    "        # Max depth reached\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return Node(label=self._majority_class(data), parent_gini=parent_gini, num_samples=num_samples)\n",
    "        # Minimum samples split reached\n",
    "        if num_samples < self.min_samples_split:\n",
    "            return Node(label=self._majority_class(data), parent_gini=parent_gini, num_samples=num_samples)\n",
    "        # No split found\n",
    "        best_gain, best_split = self._find_best_split(data)\n",
    "        if best_gain == 0:\n",
    "            return Node(label=self._majority_class(data), parent_gini=parent_gini, num_samples=num_samples)\n",
    "\n",
    "        # Put left and right data into the tree\n",
    "        if best_split[\"type\"] == \"binary\":\n",
    "            remaining_left = best_split[\"left\"]\n",
    "            remaining_right = best_split[\"right\"]\n",
    "        else:\n",
    "            remaining_left = best_split[\"left\"]\n",
    "            remaining_right = best_split[\"right\"]\n",
    "        # Recursion\n",
    "        left_tree = self._build_tree(remaining_left, depth + 1)\n",
    "        right_tree = self._build_tree(remaining_right, depth + 1)\n",
    "        return Node(\n",
    "            left=left_tree,\n",
    "            right=right_tree,\n",
    "            feature=best_split[\"feature\"],\n",
    "            threshold=best_split[\"threshold\"], \n",
    "            parent_gini=parent_gini,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "\n",
    "    def _predict_row(self, node, row):\n",
    "        '''\n",
    "        recursively predict the row\n",
    "        params node: the node\n",
    "        params row: the row\n",
    "        return: the prediction\n",
    "        '''\n",
    "        if node.is_leaf():\n",
    "            return node.label\n",
    "        else:\n",
    "            if row[node.feature] <= node.threshold:\n",
    "                return self._predict_row(node.left, row)\n",
    "            else:\n",
    "                return self._predict_row(node.right, row)\n",
    "    \n",
    "    def _count_leaves(self, node):\n",
    "        '''Helper function to count the number of leaves in a subtree'''\n",
    "        if node.is_leaf():\n",
    "            return 1\n",
    "        else:\n",
    "            return self._count_leaves(node.left) + self._count_leaves(node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for Node\n",
    "def test_Node_1():\n",
    "    # Check if the node is a leaf node\n",
    "    leaf_node = Node(label=1)\n",
    "    assert leaf_node.is_leaf(), \"Leaf node should be a leaf.\"\n",
    "    # Check if the node is a decision node\n",
    "    decision_node = Node(left=\"LeftNode\", right=\"RightNode\", feature=2, threshold=0.5)\n",
    "    assert not decision_node.is_leaf(), \"Decision node should not be a leaf.\"\n",
    "    print(\"Node tests 1 passed.\")\n",
    "\n",
    "def test_Node_2():\n",
    "    # Check if the node is a leaf node\n",
    "    leaf_node = Node(label=\"hellow\")\n",
    "    assert leaf_node.is_leaf(), \"Leaf node should be a leaf.\"\n",
    "    # Check if the node is a decision node\n",
    "    decision_node = Node(left=\"LeftNode\", right=\"RightNode\", feature=2, threshold=0.5)\n",
    "    assert not decision_node.is_leaf(), \"Decision node should not be a leaf.\"\n",
    "    print(\"Node tests 2 passed.\")\n",
    "\n",
    "def test_fit_1():\n",
    "    data = np.array([[1, 0], [2, 1], [3, 0], [4, 1]])\n",
    "    cart = CART(max_depth=1)\n",
    "    cart.fit(data)\n",
    "    print(\"test fit 1 passed\")\n",
    "    assert cart.tree is not None, \"Tree should not be None after fitting.\"\n",
    "\n",
    "def test_fit_2():\n",
    "    data = np.array([]).reshape(0, 8)\n",
    "    cart = CART()\n",
    "    try:\n",
    "        cart.fit(data)\n",
    "        assert False, \"Fitting empty data should raise an error.\"\n",
    "    except:\n",
    "        pass\n",
    "    print(\"test fit 2 passed\")\n",
    "\n",
    "def test_loss_1():\n",
    "    data = np.array([[1, 0], [2, 1], [3, 0], [4, 1]])\n",
    "    cart = CART(max_depth=4)\n",
    "    cart.fit(data)\n",
    "    loss = cart.loss(data)\n",
    "    assert loss == 0, \"Loss calculation is incorrect.\"\n",
    "    print(\"test loss 1 passed\")\n",
    "\n",
    "def test_loss_2():\n",
    "    train_data = np.array([[1, 0], [2, 1]])\n",
    "    test_data = np.array([[3, 0], [4, 1]])\n",
    "    cart = CART(max_depth=1)\n",
    "    cart.fit(train_data)\n",
    "    loss = cart.loss(test_data)\n",
    "    assert loss == 0.5, \"Loss calculation is incorrect.\"\n",
    "    print(\"test loss 2 passed\")\n",
    "\n",
    "def test_accuracy_1():\n",
    "    data = np.array([[1, 0], [2, 1], [3, 0], [4, 1]])\n",
    "    cart = CART(max_depth=4)\n",
    "    cart.fit(data)\n",
    "    acc = cart.accuracy(data)\n",
    "    assert acc == 1, \"Accuracy calculation is incorrect.\"\n",
    "    print(\"test accuracy 1 passed\")\n",
    "\n",
    "def test_accuracy_2():\n",
    "    train_data = np.array([[1, 0], [2, 1]])\n",
    "    test_data = np.array([[3, 0], [4, 1]])\n",
    "    cart = CART(max_depth=1)\n",
    "    cart.fit(train_data)\n",
    "    acc = cart.accuracy(test_data)\n",
    "    assert acc == 0.5, \"Accuracy calculation is incorrect.\"\n",
    "    print(\"test accuracy 2 passed\")\n",
    "\n",
    "# Tests for loss and accuracy\n",
    "def test_loss_acc():\n",
    "    tree = Node(left=Node(label=1), right=Node(label=0), feature=0, threshold=1.5)\n",
    "    cart = CART()\n",
    "    cart.tree = tree\n",
    "    data = np.array([\n",
    "        [1, 2, 1],\n",
    "        [2, 3, 0],\n",
    "        [0.5, 1, 1],\n",
    "        [3, 4, 0]\n",
    "    ])\n",
    "    # Check if loss = 0 and accuracy = 1\n",
    "    assert cart.loss(data) == 0, \"Loss calculation is incorrect.\"\n",
    "    assert cart.accuracy(data) == 1, \"Accuracy calculation is incorrect.\"\n",
    "    print(\"Loss and accuracy tests passed.\")\n",
    "\n",
    "def test_predict_1():\n",
    "    # Check if a single row prediction is correct\n",
    "    tree = Node(left=Node(label=1), right=Node(label=0), feature=0, threshold=1.5)\n",
    "    cart = CART()\n",
    "    row = np.array([1, 2])  # Expected: left -> 1\n",
    "    pred = cart._predict_row(tree, row)\n",
    "    assert pred == 1, \"Prediction for single row is incorrect.\"\n",
    "    # Check predictions for a dataset\n",
    "    cart.tree = tree\n",
    "    data = np.array([\n",
    "        [1, 2],\n",
    "        [2, 3]\n",
    "    ])\n",
    "    preds = cart.predict(data)\n",
    "    assert np.array_equal(preds, [1, 0]), \"Batch predictions are incorrect.\"\n",
    "    print(\"Predict test 1 passed.\")\n",
    "\n",
    "def test_predict_2():\n",
    "    train_data = np.array([[1, 0],\n",
    "                           [2, 1]])\n",
    "    test_data = np.array([[6], [101]])\n",
    "    cart = CART(max_depth=2)\n",
    "    cart.fit(train_data)\n",
    "    try:\n",
    "        cart.predict(test_data)\n",
    "        assert False, \"Predictions should raise an error if test data has different number of features.\"\n",
    "    except:\n",
    "        pass\n",
    "    print(\"Predict test 2 passed.\")\n",
    "\n",
    "# Tests for _find_best_split\n",
    "def test_find_best_split():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [2, 3.5, 1],\n",
    "        [1.5, 2, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_split = cart._find_best_split(data)\n",
    "    # Check a valid split is found and the split is correct\n",
    "    # Should not split on the target but split on one of the continuous features\n",
    "    assert best_split is not None, \"Best split should not be None.\"\n",
    "    assert best_split[\"type\"] == \"continuous\", \"Best split should be continuous.\"\n",
    "    assert best_split[\"feature\"] != 2, \"Best split should not be on the target.\"\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [1, 2.5, 0],\n",
    "        [1, 2.5, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_split = cart._find_best_split(data)\n",
    "    # Check that no split should be found if all features are the same\n",
    "    assert best_split is None, \"Best split should be None.\"\n",
    "    print(\"Find best split tests passed.\")\n",
    "\n",
    "# Tests for Gini impurity calculations\n",
    "def test_gini_1():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [2, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    gini = cart._gini_for_node(data)\n",
    "    assert abs(gini - 0.5) < 1e-6, \"Gini impurity for node is incorrect.\"\n",
    "    left = data[:1]\n",
    "    right = data[1:]\n",
    "    gini = cart._gini_for_split(data, left, right)\n",
    "    assert abs(gini - 0) < 1e-6, \"Gini impurity for split is incorrect.\"\n",
    "    print(\"Gini test 1 passed.\")\n",
    "\n",
    "def test_gini_2():\n",
    "    data = np.array([\n",
    "        [1, 0], [2, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    gini = cart._gini_for_node(data)\n",
    "    assert gini == 0, \"Gini impurity for node is incorrect.\"\n",
    "    left = data[:1]\n",
    "    right = data[1:]\n",
    "    gini = cart._gini_for_split(data, left, right)\n",
    "    assert abs(gini) < 1e-9, \"Gini impurity for split is incorrect.\"\n",
    "    print(\"Gini test 2 passed.\")\n",
    "\n",
    "def test_split_1():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [1.5, 2, 0],\n",
    "        [2, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    left, right = cart._split(data, 0, 1.25)\n",
    "    # Check if the split is correct\n",
    "    assert len(left) == 1, \"Left split should have 1 row.\"\n",
    "    assert len(right) == 2, \"Right split should have 2 rows.\"\n",
    "    print(\"Split test 1 passed.\")\n",
    "\n",
    "def test_split_2():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [1.5, 2, 0],\n",
    "        [2, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    left, right = cart._split(data, 1, -1)\n",
    "    # Check if the split is correct\n",
    "    assert len(left) == 0, \"Left split should have 0 rows.\"\n",
    "    assert len(right) == 3, \"Right split should have 3 row.\"\n",
    "    print(\"Split test 2 passed.\")\n",
    "\n",
    "def test_best_split_1():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [1.5, 2, 0],\n",
    "        [2, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_gain, best_split = cart._find_best_split(data)\n",
    "    best_gain = round(best_gain, 5)\n",
    "    assert best_gain == 0.44444, \"Best gain is incorrect.\"\n",
    "    assert best_split[\"feature\"] == 1, \"Best split feature is incorrect.\"\n",
    "    assert best_split[\"threshold\"] == 3, \"Best split threshold is incorrect.\"\n",
    "    print(\"Best split test 1 passed.\")\n",
    "\n",
    "def test_best_split_2():\n",
    "    data = np.array([\n",
    "        [2, 2, 0],\n",
    "        [2.5, 2.5, 0],\n",
    "        [3.5, 3.5, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_gain, best_split = cart._find_best_split(data)\n",
    "    best_gain = round(best_gain, 5)\n",
    "    assert best_gain == 0, \"Best gain is incorrect.\"\n",
    "    assert best_split[\"feature\"] == 1, \"Best split feature is incorrect.\"\n",
    "    assert best_split[\"threshold\"] == 3, \"Best split threshold is incorrect.\"\n",
    "    print(\"Best split test 2 passed.\")\n",
    "\n",
    "def test_majority_class_1():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [1.5, 2, 0],\n",
    "        [2, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    majority = cart._majority_class(data)\n",
    "    assert majority == 0, \"Majority class is incorrect.\"\n",
    "    print(\"Majority class test 1 passed.\")\n",
    "\n",
    "def test_majority_class_2():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 1],\n",
    "        [1.5, 2, 1],\n",
    "        [2, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    majority = cart._majority_class(data)\n",
    "    assert majority == 1, \"Majority class is incorrect.\"\n",
    "    print(\"Majority class test 2 passed.\")\n",
    "\n",
    "def test_build_tree_1():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [1.5, 2, 0],\n",
    "        [2, 3.5, 1],\n",
    "    ])\n",
    "    cart = CART(max_depth=2)\n",
    "    tree = cart._build_tree(data)\n",
    "    # Check if the tree is built correctly\n",
    "    assert tree.feature == 1, \"Root feature is incorrect.\"\n",
    "    assert tree.threshold == 3, \"Root threshold is incorrect.\"\n",
    "    assert tree.left.label == 0, \"Left leaf label is incorrect.\"\n",
    "    assert tree.right.label == 1, \"Right leaf label is incorrect.\"\n",
    "    print(\"Build tree test 1 passed.\")\n",
    "\n",
    "def test_build_tree_2():\n",
    "    data = np.array([\n",
    "        [1, 2.5, 0],\n",
    "        [2, 3.5, 0],\n",
    "        [1.5, 2, 0]\n",
    "    ])\n",
    "    cart = CART(max_depth=2)\n",
    "    tree = cart._build_tree(data)\n",
    "    # Check if the tree is built correctly\n",
    "    assert tree.label == 0, \"Root label is incorrect.\"\n",
    "    print(\"Build tree test 2 passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node tests 1 passed.\n",
      "Node tests 2 passed.\n",
      "test fit 1 passed\n",
      "test fit 2 passed\n",
      "test loss 1 passed\n",
      "test loss 2 passed\n",
      "Loss and accuracy tests passed.\n",
      "test accuracy 1 passed\n",
      "test accuracy 2 passed\n",
      "Predict test 1 passed.\n",
      "Predict test 2 passed.\n",
      "Gini test 1 passed.\n",
      "Gini test 2 passed.\n",
      "Split test 1 passed.\n",
      "Split test 2 passed.\n",
      "Best split test 1 passed.\n",
      "Best split test 2 passed.\n",
      "Majority class test 1 passed.\n",
      "Majority class test 2 passed.\n",
      "Build tree test 1 passed.\n",
      "Build tree test 2 passed.\n"
     ]
    }
   ],
   "source": [
    "# Below two functions are testing the Node to take numerical and string labels, and they should be leaf nodes\n",
    "# Edge case: numerical and string labels\n",
    "test_Node_1()\n",
    "test_Node_2()\n",
    "\n",
    "# Below two functionare testing the fit function. Tets if fit builds a tree from small dataset or empty dataset\n",
    "# Edge case: small and empty dataset\n",
    "test_fit_1()\n",
    "test_fit_2()\n",
    "\n",
    "# Below functions are testing the loss function.\n",
    "# Edge case: loss function with 0 loss and 0.5 loss and multi-dimensional data\n",
    "test_loss_1()\n",
    "test_loss_2()\n",
    "test_loss_acc() # this test both loss and accuracy for multi-dimensional data\n",
    "\n",
    "# Below functions are testing the accuracy function.\n",
    "# Edge case: accuracy function with 1 accuracy and 0.5 accuracy\n",
    "test_accuracy_1()\n",
    "test_accuracy_2()\n",
    "\n",
    "# testing for predict\n",
    "# Edge case: single row prediction and batch prediction and unseen data\n",
    "test_predict_1()\n",
    "test_predict_2()\n",
    "\n",
    "# Below functionare testing the _gini_for_node and _gini_for_split function.\n",
    "# Edge case: gini for node and split with 0 gini and multi-dimensional data\n",
    "test_gini_1()\n",
    "test_gini_2()\n",
    "\n",
    "# Below function is testing the _split function.\n",
    "# Edge case: split function with 1 row and 0 row, with a threshold being negative or below every value\n",
    "test_split_1()\n",
    "test_split_2()\n",
    "\n",
    "# Below function is testing the _find_best_split function.\n",
    "# Edge case: best split with 0 gain and 0.44444 gain, and there should be a tie, choose later feature.\n",
    "test_best_split_1()\n",
    "test_best_split_2()\n",
    "\n",
    "# Below function are for majority class\n",
    "# Edge case: multiple class vs a single class\n",
    "test_majority_class_1()\n",
    "test_majority_class_2()\n",
    "\n",
    "# Below functions are for testing build_tree\n",
    "# Edge case: build a simple tree with depth 2 and a tree with all the same features\n",
    "# also tested if the data is not in order\n",
    "test_build_tree_1()\n",
    "test_build_tree_2()\n",
    "\n",
    "# Below functions are for pruning the tree\n",
    "def test_prune_1():\n",
    "    pass\n",
    "def test_prune_2():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to unit test for prune because those two are a function that calss _prune. So we can just test those functions instead. And predict_row and count leaves are not tested because they are just function that recursively gets the label or could leaves number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. meanxai, 2023. [MXML-2-09] Decision Trees [9/11] - CART, Cost Complexity Pruning (CCP). YouTube. Available at: https://www.youtube.com/watch?v=my3ljAS5UUM&t=845s (Accessed: 12 December 2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
