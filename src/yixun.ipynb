{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from graphviz import Digraph\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_size=0.2, random_state=2060):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets randomly.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D numpy array, the entire dataset.\n",
    "    - test_size: float, the proportion of the data to include in the test split.\n",
    "    - random_state: int, the seed used by the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - train_data: 2D numpy array, the training set.\n",
    "    - test_data: 2D numpy array, the testing set.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    n_samples = data.shape[0]\n",
    "    indices = np.random.permutation(n_samples) # shuffling\n",
    "    test_size = int(n_samples * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    train_data = data[train_indices]\n",
    "    test_data = data[test_indices]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    Constructor for the Node class\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, label=None, feature=None, threshold=None, parent_gini=None, node_gini=None, num_samples=None, class_counts=None):\n",
    "        self.left = left # to a left node\n",
    "        self.right = right # to a right node\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.parent_gini = parent_gini\n",
    "        self.node_gini = node_gini\n",
    "        self.num_samples = num_samples # data points in this node\n",
    "        self.class_counts = class_counts\n",
    "\n",
    "    def is_leaf(self):\n",
    "        '''\n",
    "        check if the node is a leaf node\n",
    "        '''\n",
    "        return self.label is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "    '''\n",
    "    Decision Tree classifier by UC Providence\n",
    "    '''\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, ccp_alpha=0.01, random_state=0):\n",
    "        if max_depth is None:\n",
    "            self.max_depth = 20\n",
    "        else:\n",
    "            self.max_depth = max_depth # set the max depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "        self.ccp_alpha = ccp_alpha # for pruning\n",
    "        self.random_state = random_state\n",
    "        if random_state is not None: # make random state deterministic\n",
    "            np.random.seed(random_state)\n",
    "            random.seed(random_state)\n",
    "        \n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        build the tree based on the data\n",
    "        '''\n",
    "        self.tree = self._build_tree(data, depth=0)\n",
    "    \n",
    "    def prune(self, ccp_alpha=0):\n",
    "        '''\n",
    "        prune the tree based on the ccp_alpha\n",
    "        '''\n",
    "        self._prune_tree(self.tree, ccp_alpha)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        '''\n",
    "        Helper function to predict the data\n",
    "        '''\n",
    "        X = data[:, :-1] # get the features\n",
    "        return np.array([self._predict_row(self.tree, row) for row in X])\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the loss\n",
    "        '''\n",
    "        preds = self.predict(data)\n",
    "        true_labels = data[:, -1] # last column is the label\n",
    "        return np.sum(preds != true_labels) / len(true_labels)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the accuracy\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "    \n",
    "    def _gini_for_node(self, data):\n",
    "        '''\n",
    "        Get the gini index for a node\n",
    "        params data: the data in the node\n",
    "        return: the gini index\n",
    "        '''\n",
    "        labels = data[:, -1] # get the last column which is the label\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probs = counts / len(labels)\n",
    "        parent_gini = 1 - np.sum(probs ** 2) # calculate the gini index\n",
    "        return parent_gini\n",
    "\n",
    "    def _gini_for_split(self, data, left, right):\n",
    "        '''\n",
    "        Get the gini index for a split\n",
    "        params data: the data in the node\n",
    "        params left: the left split\n",
    "        params right: the right split\n",
    "        return: the gini index\n",
    "        '''\n",
    "        # calc the total size\n",
    "        total_size = len(data)\n",
    "        left_size = len(left)\n",
    "        right_size = len(right)\n",
    "        # calc the gini index for the left and right\n",
    "        left_gini = self._gini_for_node(left)\n",
    "        right_gini = self._gini_for_node(right)\n",
    "        # calc the weighted gini index\n",
    "        weighted_gini = (left_size / total_size) * left_gini + (right_size / total_size) * right_gini\n",
    "        return weighted_gini\n",
    "\n",
    "    def _split(self, data, feature_index, threshold):\n",
    "        '''\n",
    "        Split the data based on the feature and threshold\n",
    "        params data: the data\n",
    "        params feature_index: the feature to split on\n",
    "        params threshold: the threshold to split on\n",
    "        return: the left and right split\n",
    "        '''\n",
    "        left = data[data[:, feature_index] <= threshold]\n",
    "        right = data[data[:, feature_index] > threshold]\n",
    "        return left, right\n",
    "\n",
    "    def _find_best_split(self, data):\n",
    "        '''\n",
    "        Find the best split for the data, traverse through each column and each average value of the values in the column to find the best split.\n",
    "        params data: the dataset\n",
    "        return: the best gain and the best split\n",
    "        '''\n",
    "        best_gain = float(\"-inf\")\n",
    "        best_split = None\n",
    "        best_split_list = [] # for ties\n",
    "        parent_gini = self._gini_for_node(data) # calc the gini index for the parent node\n",
    "        n_features = data.shape[1] - 1\n",
    "        for feature in range(n_features): # traverse through each feature\n",
    "            unique_values = np.unique(data[:, feature])\n",
    "            sorted_values = np.sort(unique_values)\n",
    "            thresholds = (sorted_values[1:] + sorted_values[:-1]) / 2 # get the average of the values\n",
    "\n",
    "            if len(thresholds) > 2:\n",
    "                # Continuous or ordinal features\n",
    "                for threshold in thresholds:\n",
    "                    left, right = self._split(data, feature, threshold)\n",
    "                    if len(left) == 0 or len(right) == 0:\n",
    "                        continue # skip if the split is empty\n",
    "                    weighted_gini = self._gini_for_split(data, left, right) # calc the weighted gini index\n",
    "                    gain = parent_gini - weighted_gini\n",
    "                    if gain > best_gain: # if the gain is better than the best gain\n",
    "                        best_gain = gain\n",
    "                        best_split_list = [{\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"continuous\"\n",
    "                        }]\n",
    "                    elif np.isclose(gain, best_gain):    \n",
    "                        best_gain = gain\n",
    "                        best_split_list.append({\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"continuous\"\n",
    "                        }) # if tied, then append to the list\n",
    "            else:\n",
    "                # Only one threshold for binary features\n",
    "                for threshold in thresholds:\n",
    "                    left, right = self._split(data, feature, threshold)\n",
    "                    if len(left) == 0 or len(right) == 0:\n",
    "                        continue\n",
    "                    weighted_gini = self._gini_for_split(data, left, right)\n",
    "                    gain = parent_gini - weighted_gini\n",
    "                    if gain > best_gain:    \n",
    "                        best_gain = gain\n",
    "                        # same for binary features but with different type\n",
    "                        best_split_list = [{\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"binary\"\n",
    "                        }]\n",
    "                    elif np.isclose(gain, best_gain):    \n",
    "                        best_gain = gain\n",
    "                        best_split_list.append({\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"gini_for_split\": weighted_gini,\n",
    "                            \"parent_gini\": parent_gini,\n",
    "                            \"gain\": gain,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"binary\"\n",
    "                        })\n",
    "        if len(best_split_list) > 1:\n",
    "            # if the best split list has more than one best split then we sort it by feature\n",
    "            #print(\"Multiple best splits found\")\n",
    "            #x = sorted(best_split_list, key=lambda x: x[\"feature\"])\n",
    "            # for i in x:\n",
    "            #     print(i[\"feature\"], i[\"threshold\"], i[\"gini_for_split\"], i[\"parent_gini\"], i[\"gain\"])\n",
    "            best_split = sorted(best_split_list, key=lambda x: x[\"feature\"])[-1]\n",
    "        else: # else we just get the first best split\n",
    "            best_split = best_split_list[0]\n",
    "        return best_gain, best_split\n",
    "\n",
    "    def _majority_class(self, data):\n",
    "        '''\n",
    "        Get the majority class in the data\n",
    "        '''\n",
    "        labels = data[:, -1] # get the labels\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        return unique_labels[np.argmax(counts)]\n",
    "\n",
    "    def _build_tree(self, data, depth=0):\n",
    "        '''\n",
    "        Build the tree recursively\n",
    "        params data: the data\n",
    "        params depth: the depth of the tree\n",
    "        return: the node and its attributes\n",
    "        '''\n",
    "        labels = data[:, -1] # label is the last column\n",
    "        num_samples = len(labels)\n",
    "        parent_gini = self._gini_for_node(data)\n",
    "\n",
    "        # Stopping conditions\n",
    "        # Having a pure node\n",
    "        if len(np.unique(labels)) == 1:\n",
    "            return Node(label=labels[0], parent_gini=parent_gini, num_samples=num_samples)\n",
    "        # Max depth reached\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return Node(label=self._majority_class(data), parent_gini=parent_gini, num_samples=num_samples)\n",
    "        # Minimum samples split reached\n",
    "        if num_samples < self.min_samples_split:\n",
    "            return Node(label=self._majority_class(data), parent_gini=parent_gini, num_samples=num_samples)\n",
    "        # No split found\n",
    "        best_gain, best_split = self._find_best_split(data)\n",
    "        if best_gain == 0:\n",
    "            return Node(label=self._majority_class(data), parent_gini=parent_gini, num_samples=num_samples)\n",
    "\n",
    "        # Put left and right data into the tree\n",
    "        if best_split[\"type\"] == \"binary\":\n",
    "            remaining_left = best_split[\"left\"]\n",
    "            remaining_right = best_split[\"right\"]\n",
    "        else:\n",
    "            remaining_left = best_split[\"left\"]\n",
    "            remaining_right = best_split[\"right\"]\n",
    "        # Recursion\n",
    "        left_tree = self._build_tree(remaining_left, depth + 1)\n",
    "        right_tree = self._build_tree(remaining_right, depth + 1)\n",
    "        return Node(\n",
    "            left=left_tree,\n",
    "            right=right_tree,\n",
    "            feature=best_split[\"feature\"],\n",
    "            threshold=best_split[\"threshold\"], \n",
    "            parent_gini=parent_gini,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "\n",
    "    def _predict_row(self, node, row):\n",
    "        '''\n",
    "        recursively predict the row\n",
    "        params node: the node\n",
    "        params row: the row\n",
    "        return: the prediction\n",
    "        '''\n",
    "        if node.is_leaf():\n",
    "            return node.label\n",
    "        else:\n",
    "            if row[node.feature] <= node.threshold:\n",
    "                return self._predict_row(node.left, row)\n",
    "            else:\n",
    "                return self._predict_row(node.right, row)\n",
    "    \n",
    "    def _count_leaves(self, node):\n",
    "        '''Helper function to count the number of leaves in a subtree'''\n",
    "        if node.is_leaf():\n",
    "            return 1\n",
    "        else:\n",
    "            return self._count_leaves(node.left) + self._count_leaves(node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for splitting\n",
    "def test_splitting():\n",
    "    # Check if the splitted sizes are correct\n",
    "    data = np.array([[i] for i in range(101)])\n",
    "    train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "    assert train_data.shape[0] == 71, \"Train set size is incorrect.\"\n",
    "    assert test_data.shape[0] == 30, \"Test set size is incorrect.\"\n",
    "    # Check if the function works well for empty data\n",
    "    data = np.array([]).reshape(0, 2)\n",
    "    train_data, test_data = train_test_split(data, test_size=0.4, random_state=42)\n",
    "    assert train_data.shape[0] == 0, \"Train set should be empty.\"\n",
    "    assert test_data.shape[0] == 0, \"Test set should be empty.\"\n",
    "    print(\"Splitting tests passed.\")\n",
    "\n",
    "# Tests for Node\n",
    "def test_Node():\n",
    "    # Check if the node is a leaf node\n",
    "    leaf_node = Node(label=1)\n",
    "    assert leaf_node.is_leaf(), \"Leaf node should be a leaf.\"\n",
    "    # Check if the node is a decision node\n",
    "    decision_node = Node(left=\"LeftNode\", right=\"RightNode\", feature=2, threshold=0.5)\n",
    "    assert not decision_node.is_leaf(), \"Decision node should not be a leaf.\"\n",
    "    print(\"Node tests passed.\")\n",
    "\n",
    "# Tests for _find_best_split\n",
    "def test_find_best_split():\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [2.0, 3.5, 1],\n",
    "        [1.5, 2.0, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_split = cart._find_best_split(data)\n",
    "    # Check a valid split is found and the split is correct\n",
    "    # Should not split on the target but split on one of the continuous features\n",
    "    assert best_split is not None, \"Best split should not be None.\"\n",
    "    assert best_split[\"type\"] == \"continuous\", \"Best split should be continuous.\"\n",
    "    assert best_split[\"feature\"] != 2, \"Best split should not be on the target.\"\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [1.0, 2.5, 0],\n",
    "        [1.0, 2.5, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_split = cart._find_best_split(data)\n",
    "    # Check that no split should be found if all features are the same\n",
    "    assert best_split is None, \"Best split should be None.\"\n",
    "    print(\"Find best split tests passed.\")\n",
    "\n",
    "# Tests for Gini impurity calculations\n",
    "def test_gini():\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [2.0, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    gini = cart._gini_for_node(data) # Expected = 0.5\n",
    "    assert abs(gini - 0.5) < 1e-6, \"Gini impurity for node is incorrect.\"\n",
    "    left = data[:1]\n",
    "    right = data[1:]\n",
    "    gini = cart._gini_for_split(data, left, right) # Expected = 0\n",
    "    assert abs(gini - 0) < 1e-6, \"Gini impurity for split is incorrect.\"\n",
    "    print(\"Gini tests passed.\")\n",
    "\n",
    "# Tests for splitting continuous features and categorical features\n",
    "def test_split_features():\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [2.0, 3.5, 1],\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 4.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    left, right = cart._split_continuous(data, 1, 3.0)\n",
    "    # Check if the split is correct for a continuous feature\n",
    "    assert len(left) == 2, \"Left split size is incorrect.\"\n",
    "    assert len(right) == 2, \"Right split size is incorrect.\"\n",
    "    left, right = cart._split_categorical(data, 2)\n",
    "    # Check if the split is correct for a categorical feature\n",
    "    assert len(left) == 2, \"Left split size is incorrect.\"\n",
    "    assert len(right) == 2, \"Right split size is incorrect.\"\n",
    "    print(\"Split features tests passed.\")\n",
    "\n",
    "# Tests for fit and _build_tree\n",
    "def test_fit():\n",
    "    data = np.array([\n",
    "        [1, 2, 0],\n",
    "        [3, 4, 1],\n",
    "        [1, 2, 0],\n",
    "        [3, 4, 1]\n",
    "    ])\n",
    "    cart = CART(max_depth=2, min_samples_split=2)\n",
    "    cart.fit(data)\n",
    "    # Check if the tree is built\n",
    "    assert cart.tree is not None, \"Tree should not be None after fitting.\"\n",
    "    assert cart.tree.feature is not None, \"Tree root should have a splitting feature.\"\n",
    "    data = np.empty((0, 3))\n",
    "    cart.fit(data)\n",
    "    # Check if the label is 0\n",
    "    assert cart.tree.label == 0, \"Incorrect label for empty data.\"\n",
    "    print(\"Fit tests passed.\")\n",
    "\n",
    "# Tests for predict\n",
    "def test_predict():\n",
    "    # Check if a single row prediction is correct\n",
    "    tree = Node(left=Node(label=1), right=Node(label=0), feature=0, threshold=1.5)\n",
    "    cart = CART()\n",
    "    row = np.array([1.0, 2.0])  # Expected: left -> 1\n",
    "    pred = cart._predict_row(tree, row)\n",
    "    assert pred == 1, \"Prediction for single row is incorrect.\"\n",
    "    # Check predictions for a dataset\n",
    "    cart.tree = tree\n",
    "    data = np.array([\n",
    "        [1.0, 2.0],\n",
    "        [2.0, 3.0]\n",
    "    ])\n",
    "    preds = cart.predict(data)\n",
    "    assert np.array_equal(preds, [1, 0]), \"Batch predictions are incorrect.\"\n",
    "    print(\"Predict tests passed.\")\n",
    "\n",
    "# Tests for loss and accuracy\n",
    "def test_loss_acc():\n",
    "    tree = Node(left=Node(label=1), right=Node(label=0), feature=0, threshold=1.5)\n",
    "    cart = CART()\n",
    "    cart.tree = tree\n",
    "    data = np.array([\n",
    "        [1.0, 2.0, 1],\n",
    "        [2.0, 3.0, 0],\n",
    "        [0.5, 1.0, 1],\n",
    "        [3.0, 4.0, 0]\n",
    "    ])\n",
    "    # Check if loss = 0 and accuracy = 1\n",
    "    assert cart.loss(data) == 0.0, \"Loss calculation is incorrect.\"\n",
    "    assert cart.accuracy(data) == 1.0, \"Accuracy calculation is incorrect.\"\n",
    "    print(\"Loss and accuracy tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting tests passed.\n",
      "Node tests passed.\n",
      "Gini tests passed.\n",
      "Predict tests passed.\n",
      "Loss and accuracy tests passed.\n"
     ]
    }
   ],
   "source": [
    "test_splitting()\n",
    "test_Node()\n",
    "# test_find_best_split()\n",
    "test_gini()\n",
    "# test_split_features()\n",
    "# test_fit()\n",
    "test_predict()\n",
    "test_loss_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.901\n",
      "Testing accuracy: 0.893\n",
      "--- START PRINT TREE ---\n",
      "Split attribute = 2; threshold = 0.500\n",
      "Left:\n",
      "  Split attribute = 11; threshold = 0.500\n",
      "  Left:\n",
      "    Split attribute = 12; threshold = 2.500\n",
      "    Left:\n",
      "      Split attribute = 8; categorical\n",
      "      Left:\n",
      "        Split attribute = 7; threshold = 96.500\n",
      "        Left:\n",
      "          Predict -> 0.0\n",
      "        Right:\n",
      "          Split attribute = 4; threshold = 316.500\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0\n",
      "      Right:\n",
      "        Split attribute = 6; threshold = 0.500\n",
      "        Left:\n",
      "          Predict -> 1.0\n",
      "        Right:\n",
      "          Split attribute = 9; threshold = 1.500\n",
      "          Left:\n",
      "            Split attribute = 3; threshold = 115.000\n",
      "            Left:\n",
      "              Predict -> 1.0\n",
      "            Right:\n",
      "              Predict -> 0.0\n",
      "          Right:\n",
      "            Predict -> 1.0\n",
      "    Right:\n",
      "      Split attribute = 9; threshold = 0.650\n",
      "      Left:\n",
      "        Split attribute = 0; threshold = 42.000\n",
      "        Left:\n",
      "          Predict -> 0.0\n",
      "        Right:\n",
      "          Split attribute = 4; threshold = 237.500\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0.0\n",
      "      Right:\n",
      "        Predict -> 0.0\n",
      "  Right:\n",
      "    Split attribute = 3; threshold = 109.000\n",
      "    Left:\n",
      "      Split attribute = 4; threshold = 233.500\n",
      "      Left:\n",
      "        Predict -> 1.0\n",
      "      Right:\n",
      "        Predict -> 0.0\n",
      "    Right:\n",
      "      Split attribute = 4; threshold = 301.500\n",
      "      Left:\n",
      "        Split attribute = 7; threshold = 105.500\n",
      "        Left:\n",
      "          Split attribute = 9; threshold = 0.600\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0.0\n",
      "        Right:\n",
      "          Predict -> 0.0\n",
      "      Right:\n",
      "        Split attribute = 4; threshold = 303.500\n",
      "        Left:\n",
      "          Predict -> 1.0\n",
      "        Right:\n",
      "          Predict -> 0.0\n",
      "Right:\n",
      "  Split attribute = 9; threshold = 1.950\n",
      "  Left:\n",
      "    Split attribute = 0; threshold = 56.500\n",
      "    Left:\n",
      "      Split attribute = 7; threshold = 142.500\n",
      "      Left:\n",
      "        Split attribute = 1; categorical\n",
      "        Left:\n",
      "          Predict -> 1.0\n",
      "        Right:\n",
      "          Split attribute = 6; threshold = 125.500\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Split attribute = 0; threshold = 43.000\n",
      "            Left:\n",
      "              Predict -> 1.0\n",
      "            Right:\n",
      "              Predict -> 0.0\n",
      "      Right:\n",
      "        Split attribute = 3; threshold = 111.000\n",
      "        Left:\n",
      "          Split attribute = 10; threshold = 0.500\n",
      "          Left:\n",
      "            Predict -> 0.0\n",
      "          Right:\n",
      "            Split attribute = 7; threshold = 152.500\n",
      "            Left:\n",
      "              Predict -> 0\n",
      "            Right:\n",
      "              Predict -> 1.0\n",
      "        Right:\n",
      "          Split attribute = 3; threshold = 182.000\n",
      "          Left:\n",
      "            Split attribute = 12; threshold = 2.500\n",
      "            Left:\n",
      "              Predict -> 1.0\n",
      "            Right:\n",
      "              Split attribute = 11; categorical\n",
      "              Left:\n",
      "                Predict -> 1.0\n",
      "              Right:\n",
      "                Predict -> 0.0\n",
      "          Right:\n",
      "            Predict -> 0.0\n",
      "    Right:\n",
      "      Split attribute = 1; categorical\n",
      "      Left:\n",
      "        Split attribute = 0; threshold = 57.500\n",
      "        Left:\n",
      "          Predict -> 0.0\n",
      "        Right:\n",
      "          Split attribute = 6; threshold = 106.000\n",
      "          Left:\n",
      "            Predict -> 1\n",
      "          Right:\n",
      "            Split attribute = 0; threshold = 59.000\n",
      "            Left:\n",
      "              Predict -> 1\n",
      "            Right:\n",
      "              Predict -> 1.0\n",
      "      Right:\n",
      "        Split attribute = 3; threshold = 245.500\n",
      "        Left:\n",
      "          Split attribute = 6; threshold = 148.000\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Split attribute = 6; threshold = 152.000\n",
      "            Left:\n",
      "              Predict -> 0.0\n",
      "            Right:\n",
      "              Split attribute = 10; threshold = 1.500\n",
      "              Left:\n",
      "                Split attribute = 1; threshold = 1.500\n",
      "                Left:\n",
      "                  Predict -> 0.0\n",
      "                Right:\n",
      "                  Predict -> 1.0\n",
      "              Right:\n",
      "                Predict -> 0.0\n",
      "        Right:\n",
      "          Split attribute = 2; threshold = 119.000\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0.0\n",
      "  Right:\n",
      "    Split attribute = 10; threshold = 0.500\n",
      "    Left:\n",
      "      Predict -> 1.0\n",
      "    Right:\n",
      "      Split attribute = 5; categorical\n",
      "      Left:\n",
      "        Split attribute = 0; threshold = 43.500\n",
      "        Left:\n",
      "          Predict -> 0\n",
      "        Right:\n",
      "          Predict -> 0.0\n",
      "      Right:\n",
      "        Predict -> 0\n",
      "--- END PRINT TREE ---\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"../data/heart.csv\", delimiter=\",\", skiprows=1)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=2060)\n",
    "\n",
    "model = CART(max_depth=10, min_samples_split=10)\n",
    "model.fit(train_data)\n",
    "train_accuracy = model.accuracy(train_data)\n",
    "test_accuracy = model.accuracy(test_data)\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Testing accuracy: {test_accuracy:.3f}\")\n",
    "model.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the CART model using scikit-learning is written by our group members, Yixun Kang and David Ning. The model use a pipeline structure, including the preprocessor and the algorithm. In the preprocessor, we used One-Hot encoder for two categorical features, `sex` and `exange` and then applied `StandardScaler()` to all features. We used K-Fold as the cross validation and 6/2/2 for train/test/validation. In the parameter grid, we tuned for `min_samples_leaf` and `max_leaf_nodes` and keep the `max_depth` and `min_samples_split` the same as in main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/heart.csv\")\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor\n",
    "cat_ftrs = [\"sex\", \"exang\"]\n",
    "num_ftrs = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"cp\", \"fbs\", \"restecg\", \"slope\", \"ca\", \"thal\"]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", categorical_transformer, cat_ftrs),\n",
    "    (\"num\", numerical_transformer, num_ftrs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML pipeline\n",
    "def MLpipe_kfold(X, y, random_states, preprocessor, ML_algo, param_grid, n_splits=5):\n",
    "    test_scores = []\n",
    "    best_models = []\n",
    "    for i, random_state in enumerate(random_states):\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        pipe = make_pipeline(preprocessor, ML_algo)\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid, cv=kf, n_jobs=-1, return_train_score=True, \n",
    "                            verbose=True, scoring=\"accuracy\")\n",
    "        grid.fit(X_other, y_other)\n",
    "        results = pd.DataFrame(grid.cv_results_)\n",
    "        best_models.append(grid)\n",
    "        y_test_pred = best_models[-1].predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_scores.append(test_accuracy)\n",
    "    return test_scores, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Average Testing Accuracy: 0.8829268292682927\n"
     ]
    }
   ],
   "source": [
    "random_states = [2060]\n",
    "ML_algo = DecisionTreeClassifier(random_state=2060, criterion=\"gini\", max_depth=10, min_samples_split=10)\n",
    "param_grid = {\n",
    "    \"decisiontreeclassifier__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"decisiontreeclassifier__max_leaf_nodes\": [5, 10]\n",
    "}\n",
    "test_scores, best_models = MLpipe_kfold(X, y, random_states, preprocessor, ML_algo, param_grid, n_splits=10)\n",
    "print(\"Average Testing Accuracy:\", np.mean(test_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
