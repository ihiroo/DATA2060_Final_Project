{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#####################################################################################################################\n",
    "# Data Processing Section\n",
    "# Helper function for preparing data for a decision tree classifiction problem. Parsing the data such\n",
    "# that for each feature, the property can only either be True or False. Label can only be 1 or 0.\n",
    "# For the chess.csv dataset won=1, nowin=0\n",
    "# In more detail:\n",
    "# Dataset with n instances, for each instance, there are m attributes. For the i-th attribute,\n",
    "# the property should be chosen from a set with size of m_i to represent the information.\n",
    "# Input: array with size of n*(m+1), the first column is the label\n",
    "# Output: array with size of n*(m_1 + m_2 + ... + m_m + 1), the first column is 1 or 0 corresponding to label\n",
    "#####################################################################################################################\n",
    "\n",
    "def get_data(filename, class_name, num_training, num_validation):\n",
    "    data = read_data(filename)\n",
    "    data = convert_to_binary_features(data, class_name)\n",
    "    return np.array(split_data(data, num_training, num_validation), dtype=object)\n",
    "\n",
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def convert_to_binary_features(data, class_name):\n",
    "    features = []\n",
    "    for feature_index in range(0, len(data[0])-1):\n",
    "        feature_values = list(set([obs[feature_index] for obs in data]))\n",
    "        feature_values.sort()\n",
    "        if len(feature_values) > 2: features.append(feature_values[:-1])\n",
    "        else: features.append([feature_values[0]])\n",
    "    new_data = []\n",
    "    for obs in data:\n",
    "        new_obs = [1 if obs[-1] == class_name else 0] # label = 1 if label in the dataset is won\n",
    "        for feature_index in range(0, len(data[0]) - 1):\n",
    "            current_feature_value = obs[feature_index]\n",
    "            for possible_feature_value in features[feature_index]:\n",
    "                new_obs.append(current_feature_value == possible_feature_value)\n",
    "        new_data.append(new_obs)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def split_data(data, num_training, num_validation):\n",
    "    random.shuffle(data)\n",
    "    # casting to a numpy array\n",
    "    data = np.array(data)\n",
    "    return data[0:num_training], data[num_training:num_training + num_validation], data[num_training + num_validation:len(data)]## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def node_score_gini(prob):\n",
    "    '''\n",
    "    Calculate the node score using the gini index of the subdataset and return it.\n",
    "    For dataset with 2 classes, C(p) = 2 * p * (1-p)\n",
    "    '''\n",
    "    gini_index = 2 * prob * (1 - prob)\n",
    "    return gini_index\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper to construct the tree structure.\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, threshold=None, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.threshold = threshold  # Add threshold attribute for continuous features\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.info = {}  # used for visualization\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        '''\n",
    "        Helper function to add to info attribute.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, validation_data=None, gain_function=node_score_gini, max_depth=40):\n",
    "        # Find majority class; set to class 0 if exactly balanced.\n",
    "        labels, counts = np.unique([row[0] for row in data], return_counts=True)\n",
    "        self.majority_class = labels[np.argmax(counts)]\n",
    "        \n",
    "        labels, counts = np.unique([row[0] for row in data], return_counts=True)\n",
    "        if np.sum(counts == np.max(counts)) > 1:\n",
    "            self.majority_class = 0\n",
    "        else:\n",
    "            self.majority_class = labels[np.argmax(counts)]\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(label=self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "\n",
    "        self._split_recurs(self.root, data, indices)\n",
    "\n",
    "        # Pruning\n",
    "        if validation_data is not None:\n",
    "            self._prune_recurs(self.root, validation_data)\n",
    "\n",
    "    def predict(self, features):\n",
    "        return self._predict_recurs(self.root, features)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "    def loss(self, data):\n",
    "        cnt = 0.0\n",
    "        test_Y = [row[0] for row in data]\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data[i])\n",
    "            if prediction != test_Y[i]:\n",
    "                cnt += 1.0\n",
    "        return cnt / len(data)\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "        split_index = node.index_split_on\n",
    "\n",
    "        if node.threshold is not None:\n",
    "            # For continuous feature, compare with threshold\n",
    "            if row[split_index] <= node.threshold:\n",
    "                return self._predict_recurs(node.left, row)\n",
    "            else:\n",
    "                return self._predict_recurs(node.right, row)\n",
    "        else:\n",
    "            # For binary feature\n",
    "            if not row[split_index]:\n",
    "                return self._predict_recurs(node.left, row)\n",
    "            else:\n",
    "                return self._predict_recurs(node.right, row)\n",
    "\n",
    "    def _prune_recurs(self, node, validation_data):\n",
    "        if not node.isleaf:\n",
    "            if node.left is not None:\n",
    "                self._prune_recurs(node.left, validation_data)\n",
    "            if node.right is not None:\n",
    "                self._prune_recurs(node.right, validation_data)\n",
    "\n",
    "            if node.left.isleaf and node.right.isleaf:\n",
    "                current_loss = self.loss(validation_data)\n",
    "                original_left, original_right = node.left, node.right\n",
    "                node.isleaf = True\n",
    "                node.left, node.right = None, None\n",
    "                new_loss = self.loss(validation_data)\n",
    "\n",
    "                if new_loss > current_loss:\n",
    "                    node.isleaf = False\n",
    "                    node.left, node.right = original_left, original_right\n",
    "\n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        y = [row[0] for row in data]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            return True, self.majority_class\n",
    "        if len(indices) == 0:\n",
    "            Labels, Counts = np.unique(y, return_counts=True)\n",
    "            Label = Labels[np.argmax(Counts)]\n",
    "            return True, Label\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return True, y[0]\n",
    "        if node.depth >= self.max_depth:\n",
    "            Labels, Counts = np.unique(y, return_counts=True)\n",
    "            Label = Labels[np.argmax(Counts)]\n",
    "            return True, Label\n",
    "\n",
    "        if y.count(0) == y.count(1):\n",
    "            return False, self.majority_class\n",
    "\n",
    "        Labels, Counts = np.unique(y, return_counts=True)\n",
    "        Label = Labels[np.argmax(Counts)]\n",
    "        return False, Label\n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "        is_terminal, label = self._is_terminal(node, data, indices)\n",
    "        if is_terminal:\n",
    "            node.isleaf = True\n",
    "            node.label = label\n",
    "            return\n",
    "\n",
    "        best_gain = -float('inf')\n",
    "        best_index = -1\n",
    "        best_threshold = None\n",
    "\n",
    "        for index in indices:\n",
    "            # Find the best threshold for the current feature\n",
    "            gain, threshold = self._find_best_split(data, index, self.gain_function)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_index = index\n",
    "                best_threshold = threshold\n",
    "\n",
    "        node.index_split_on = best_index\n",
    "        node.threshold = best_threshold\n",
    "        node._set_info(best_gain, len(data))\n",
    "\n",
    "        # Split the data based on the best threshold\n",
    "        if best_threshold is not None:\n",
    "            left_data = [row for row in data if row[best_index] <= best_threshold]\n",
    "            right_data = [row for row in data if row[best_index] > best_threshold]\n",
    "        else:\n",
    "            left_data = [row for row in data if row[best_index] == 0]\n",
    "            right_data = [row for row in data if row[best_index] == 1]\n",
    "\n",
    "        node.left = Node(depth=node.depth + 1)\n",
    "        node.right = Node(depth=node.depth + 1)\n",
    "        remaining_indices = copy.deepcopy(indices)\n",
    "        self._split_recurs(node.left, left_data, remaining_indices)\n",
    "        self._split_recurs(node.right, right_data, remaining_indices)\n",
    "\n",
    "    def _find_best_split(self, data, split_index, gain_function):\n",
    "        \"\"\"\n",
    "        Find the best threshold for a given feature.\n",
    "        Returns the best gain and corresponding threshold.\n",
    "        \"\"\"\n",
    "        unique_values = sorted(set([row[split_index] for row in data]))\n",
    "        best_gain = -float('inf')\n",
    "        best_threshold = None\n",
    "\n",
    "        for i in range(1, len(unique_values)):\n",
    "            threshold = (unique_values[i - 1] + unique_values[i]) / 2\n",
    "            left_data = [row for row in data if row[split_index] <= threshold]\n",
    "            right_data = [row for row in data if row[split_index] > threshold]\n",
    "\n",
    "            if len(left_data) > 0 and len(right_data) > 0:\n",
    "                left_prob = sum([row[0] for row in left_data]) / len(left_data)\n",
    "                right_prob = sum([row[0] for row in right_data]) / len(right_data)\n",
    "\n",
    "                gain = gain_function(sum([row[0] for row in data]) / len(data))\n",
    "                gain -= len(left_data) / len(data) * gain_function(left_prob)\n",
    "                gain -= len(right_data) / len(data) * gain_function(right_prob)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_gain, best_threshold\n",
    "\n",
    "    def _calc_gain(self, data, split_index, gain_function):\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [row[split_index] for row in data]\n",
    "\n",
    "        if len(y) != 0 and len(xi) != 0:\n",
    "            Py1 = sum(y) / len(y)\n",
    "            Pxi1 = sum(xi) / len(xi)\n",
    "            Pxi0 = 1 - Pxi1\n",
    "\n",
    "            if sum(xi) != 0:\n",
    "                Py1_given_xi1 = sum([1 for i in range(len(y)) if y[i] == 1 and xi[i] == 1]) / sum(xi)\n",
    "            else:\n",
    "                Py1_given_xi1 = 0\n",
    "\n",
    "            if sum([1 for _ in xi if _ == 0]) != 0:\n",
    "                Py0_given_xi0 = sum([1 for i in range(len(y)) if y[i] == 0 and xi[i] == 0]) / sum([1 for _ in xi if _ == 0])\n",
    "            else:\n",
    "                Py0_given_xi0 = 0\n",
    "\n",
    "            parent_cost = gain_function(Py1)\n",
    "            left_cost = Pxi1 * gain_function(Py1_given_xi1)\n",
    "            right_cost = Pxi0 * gain_function(Py0_given_xi0)\n",
    "\n",
    "            gain = parent_cost - left_cost - right_cost\n",
    "        else:\n",
    "            gain = 0\n",
    "        return gain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def node_score_gini(probabilities):\n",
    "    '''\n",
    "    Calculate the node score using the gini index of the subdataset and return it.\n",
    "    For datasets with multiple classes, Gini(p) = 1 - sum(p_i^2)\n",
    "    '''\n",
    "    return 1 - sum([p ** 2 for p in probabilities])\n",
    "\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper to construct the tree structure.\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, threshold=None, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.threshold = threshold  # Add threshold attribute for continuous features\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.info = {}  # used for visualization\n",
    "\n",
    "    def set_info(self, gain, num_samples):\n",
    "        '''\n",
    "        Helper function to add to info attribute.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, validation_data=None, gain_function=node_score_gini, max_depth=40):\n",
    "        labels, counts = np.unique([row[0] for row in data], return_counts=True)\n",
    "        self.majority_class = labels[np.argmax(counts)] if len(counts) > 0 else 0\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(label=self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "        self._split_recursively(self.root, data, indices)\n",
    "\n",
    "        # Pruning\n",
    "        if validation_data is not None:\n",
    "            self._prune_recursively(self.root, validation_data)\n",
    "\n",
    "    def predict(self, features):\n",
    "        return self._predict_recursive(self.root, features)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "    def loss(self, data):\n",
    "        cnt = 0.0\n",
    "        for row in data:\n",
    "            prediction = self.predict(row)\n",
    "            if prediction != row[0]:\n",
    "                cnt += 1.0\n",
    "        return cnt / len(data)\n",
    "\n",
    "    def _predict_recursive(self, node, row):\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "\n",
    "        split_index = node.index_split_on\n",
    "\n",
    "        if node.threshold is not None:\n",
    "            # For continuous feature, compare with threshold\n",
    "            if row[split_index] <= node.threshold:\n",
    "                return self._predict_recursive(node.left, row)\n",
    "            else:\n",
    "                return self._predict_recursive(node.right, row)\n",
    "        else:\n",
    "            # For binary feature\n",
    "            if not row[split_index]:\n",
    "                return self._predict_recursive(node.left, row)\n",
    "            else:\n",
    "                return self._predict_recursive(node.right, row)\n",
    "\n",
    "    def _prune_recursively(self, node, validation_data):\n",
    "        if not node.isleaf:\n",
    "            if node.left:\n",
    "                self._prune_recursively(node.left, validation_data)\n",
    "            if node.right:\n",
    "                self._prune_recursively(node.right, validation_data)\n",
    "\n",
    "            if node.left.isleaf and node.right.isleaf:\n",
    "                current_loss = self.loss(validation_data)\n",
    "                original_left, original_right = node.left, node.right\n",
    "                node.isleaf = True\n",
    "                node.left, node.right = None, None\n",
    "                new_loss = self.loss(validation_data)\n",
    "\n",
    "                if new_loss > current_loss:\n",
    "                    node.isleaf = False\n",
    "                    node.left, node.right = original_left, original_right\n",
    "\n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        y = [row[0] for row in data]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            return True, self.majority_class\n",
    "        if len(indices) == 0 or len(np.unique(y)) == 1 or node.depth >= self.max_depth:\n",
    "            label = max(set(y), key=y.count) if len(y) > 0 else self.majority_class\n",
    "            return True, label\n",
    "        return False, max(set(y), key=y.count)\n",
    "\n",
    "    def _split_recursively(self, node, data, indices):\n",
    "        is_terminal, label = self._is_terminal(node, data, indices)\n",
    "        if is_terminal:\n",
    "            node.isleaf = True\n",
    "            node.label = label\n",
    "            return\n",
    "\n",
    "        best_gain, best_index, best_threshold = -float('inf'), -1, None\n",
    "\n",
    "        for index in indices:\n",
    "            gain, threshold = self._find_best_split(data, index)\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_index, best_threshold = gain, index, threshold\n",
    "\n",
    "        node.index_split_on = best_index\n",
    "        node.threshold = best_threshold\n",
    "        node.set_info(best_gain, len(data))\n",
    "\n",
    "        left_data, right_data = self._split_data(data, best_index, best_threshold)\n",
    "\n",
    "        node.left = Node(depth=node.depth + 1)\n",
    "        node.right = Node(depth=node.depth + 1)\n",
    "        self._split_recursively(node.left, left_data, copy.deepcopy(indices))\n",
    "        self._split_recursively(node.right, right_data, copy.deepcopy(indices))\n",
    "\n",
    "    def _split_data(self, data, split_index, threshold):\n",
    "        if threshold is not None:\n",
    "            left_data = [row for row in data if row[split_index] <= threshold]\n",
    "            right_data = [row for row in data if row[split_index] > threshold]\n",
    "        else:\n",
    "            left_data = [row for row in data if row[split_index] == 0]\n",
    "            right_data = [row for row in data if row[split_index] == 1]\n",
    "        return left_data, right_data\n",
    "\n",
    "    def _find_best_split(self, data, split_index):\n",
    "        unique_values = sorted(set([row[split_index] for row in data]))\n",
    "        best_gain, best_threshold = -float('inf'), None\n",
    "\n",
    "        # Use a more efficient approach to find the best threshold\n",
    "        if len(unique_values) > 10:\n",
    "            thresholds = np.linspace(unique_values[0], unique_values[-1], num=10)\n",
    "        else:\n",
    "            thresholds = [(unique_values[i - 1] + unique_values[i]) / 2 for i in range(1, len(unique_values))]\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            left_data, right_data = self._split_data(data, split_index, threshold)\n",
    "\n",
    "            if len(left_data) > 0 and len(right_data) > 0:\n",
    "                gain = self._calculate_gain(data, left_data, right_data)\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_threshold = gain, threshold\n",
    "\n",
    "        return best_gain, best_threshold\n",
    "\n",
    "    def _calculate_gain(self, parent_data, left_data, right_data):\n",
    "        # Calculate the probability distribution for parent, left, and right nodes\n",
    "        parent_labels, parent_counts = np.unique([row[0] for row in parent_data], return_counts=True)\n",
    "        parent_probabilities = parent_counts / len(parent_data)\n",
    "\n",
    "        left_labels, left_counts = np.unique([row[0] for row in left_data], return_counts=True)\n",
    "        left_probabilities = left_counts / len(left_data)\n",
    "\n",
    "        right_labels, right_counts = np.unique([row[0] for row in right_data], return_counts=True)\n",
    "        right_probabilities = right_counts / len(right_data)\n",
    "\n",
    "        # Calculate Gini gain\n",
    "        gain = self.gain_function(parent_probabilities)\n",
    "        gain -= (len(left_data) / len(parent_data)) * self.gain_function(left_probabilities)\n",
    "        gain -= (len(right_data) / len(parent_data)) * self.gain_function(right_probabilities)\n",
    "\n",
    "        return gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/heart.csv\n",
      "Gini Index \n",
      " train_loss_not_pruned is 0.0 \n",
      " test_loss_not_pruned is 0.1927710843373494 \n",
      " train_loss_pruned is 0.2 \n",
      " test_loss_pruned is 0.21686746987951808 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_dataset(filename, class_name, num_training, num_validation):\n",
    "    train_data, validation_data, test_data = get_data(filename, class_name, num_training, num_validation)\n",
    "\n",
    "    # TODO: Print 12 loss values associated with the dataset.\n",
    "    # For each measure of gain (training error, entropy, gini):\n",
    "    #      (a) Print average training loss (not-pruned)\n",
    "    #      (b) Print average test loss (not-pruned)\n",
    "    #      (c) Print average training loss (pruned)\n",
    "    #      (d) Print average test loss (pruned)\n",
    "\n",
    "    gain_functions = {\"Gini Index\": node_score_gini}\n",
    "\n",
    "    results = []\n",
    "    print(f'{filename}')\n",
    "    for gain_name, gain_function in gain_functions.items():\n",
    "        tree_not_pruned = DecisionTree(data=train_data, gain_function=gain_function, max_depth=10)\n",
    "        train_loss_not_pruned = tree_not_pruned.loss(train_data)\n",
    "        test_loss_not_pruned = tree_not_pruned.loss(test_data)\n",
    "        tree_pruned = DecisionTree(data=train_data, validation_data=validation_data, gain_function=gain_function, max_depth=10)\n",
    "        train_loss_pruned = tree_pruned.loss(train_data)\n",
    "        test_loss_pruned = tree_pruned.loss(test_data)\n",
    "        print(f'{gain_name}', '\\n' , 'train_loss_not_pruned is', f'{train_loss_not_pruned}', '\\n' , 'test_loss_not_pruned is', f'{test_loss_not_pruned}', '\\n' , 'train_loss_pruned is', f'{train_loss_pruned}', '\\n' , 'test_loss_pruned is', f'{test_loss_pruned}', '\\n')\n",
    "        results.append((gain_name, train_loss_not_pruned, test_loss_not_pruned, train_loss_pruned, test_loss_pruned))\n",
    "\n",
    "explore_dataset('archive/heart.csv', '1', num_training=120, num_validation=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
      "DATA2060_Final_Project/data/transformed_iris.csv\n",
      "Gini Index \n",
      " train_loss_not_pruned is 0.014285714285714285 \n",
      " test_loss_not_pruned is 0.0 \n",
      " train_loss_pruned is 0.014285714285714285 \n",
      " test_loss_pruned is 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def data_transform(file_name, new_file_name):\n",
    "    with open(file_name, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)\n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for row in reader:\n",
    "            labels.append(row[-1])\n",
    "            data.append(row[1:-1])\n",
    "            \n",
    "    unique_labels = list(set(labels))\n",
    "    label_to_number = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    encoded_labels = [label_to_number[label] for label in labels]\n",
    "    for i in range(len(data)):\n",
    "        data[i].append(encoded_labels[i])\n",
    "        \n",
    "    with open(new_file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers[1:])\n",
    "        print(headers[1:])\n",
    "        writer.writerows(data)\n",
    "    \n",
    "\n",
    "data_transform('DATA2060_Final_Project/data/iris.csv', 'DATA2060_Final_Project/data/transformed_iris.csv')\n",
    "explore_dataset('DATA2060_Final_Project/data/transformed_iris.csv', 'Species', num_training=70, num_validation=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DATA_2060",
   "language": "python",
   "name": "data_2060"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
