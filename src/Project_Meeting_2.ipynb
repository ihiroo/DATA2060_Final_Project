{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_size=0.4, random_state=2060):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets randomly.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D numpy array, the entire dataset.\n",
    "    - test_size: float, the proportion of the data to include in the test split.\n",
    "    - random_state: int, the seed used by the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - train_data: 2D numpy array, the training set.\n",
    "    - test_data: 2D numpy array, the testing set.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    n_samples = data.shape[0]\n",
    "    indices = np.random.permutation(n_samples) # shuffling\n",
    "    test_size = int(n_samples * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    train_data = data[train_indices]\n",
    "    test_data = data[test_indices]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a node in the decision tree. Each node can be one of the following:\n",
    "    - Decision node: splits the data.\n",
    "    - Leaf node: predicts the label.\n",
    "\n",
    "    Attributes:\n",
    "    - left: none for leaf nodes.\n",
    "    - right: none for leaf nodes.\n",
    "    - label: none for decision nodes.\n",
    "    - feature: none for leaf nodes.\n",
    "    - threshold: none for leaf nodes or categorical splits.\n",
    "    \"\"\"\n",
    "    def __init__(self, left=None, right=None, label=None, feature=None, threshold=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Check if a node is a leaf node.\n",
    "\n",
    "        Returns:\n",
    "        - A boolean value indicating whether the node is a leaf node. True if it is a leaf node, False otherwise.\n",
    "        \"\"\"\n",
    "        return self.label is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:\n",
    "    # TODO: visit hw5? set major = 0 if balanced?\n",
    "    # TODO: if training set is empty, should we take predict all labels in the test set to be the major class?\n",
    "    \"\"\"\n",
    "    A class to implement a CART decision tree.\n",
    "\n",
    "    Attributes:\n",
    "    - max_depth: int, the maximum depth of the tree.\n",
    "    - min_samples_split: int, the minimum number of samples required to split an internal node.\n",
    "    - tree: Node, the node of the decision tree. It can be None if the tree is not built yet.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=10, min_samples_split=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Fit the decision tree to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - data: 2D numpy array, the entire training dataset where the last column is the target variable.\n",
    "            - Note: below all \"data\" refers to the entire training data, including the target variable.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(data, depth=0)\n",
    "\n",
    "    def _build_tree(self, data, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the tree.\n",
    "        This method constructs the tree by splitting the dataset at each node based on the feature and threshold\n",
    "        that minimize the Gini impurity. It stops the recursion when one of the following conditions is met:\n",
    "        - All labels in the current node are the same.\n",
    "        - The maximum depth is reached.\n",
    "        - The minimum number of samples required to split an internal node is not met.\n",
    "\n",
    "        Parameters:\n",
    "        - data: 2D numpy array, the entire training dataset at the current node.\n",
    "        - depth: int, the current depth of the tree.\n",
    "        \n",
    "        Returns:\n",
    "        - Node: a decision node or leaf node of the tree.\n",
    "        \"\"\"\n",
    "        # If the input training dataset is empty, then guess all labels to be 0\n",
    "        if len(data) == 0:\n",
    "            return Node(label=0)\n",
    "        \n",
    "        labels = data[:, -1]\n",
    "        # Stopping conditions\n",
    "        # All labels are the same\n",
    "        if len(np.unique(labels)) == 1:\n",
    "            return Node(label=labels[0])\n",
    "        # Max depth or minimum split size is reached\n",
    "        # TODO: check if we need to add the = case?\n",
    "        if depth >= self.max_depth or len(data) < self.min_samples_split:\n",
    "            major = np.bincount(labels.astype(int)).argmax()\n",
    "            return Node(label=major) # return the majority class\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._find_best_split(data)\n",
    "        # No valid split\n",
    "        if not best_split:\n",
    "            major = np.bincount(labels.astype(int)).argmax()\n",
    "            return Node(label=major) # return the majority class\n",
    "        # Remove the splitted categorical feature\n",
    "        if best_split[\"type\"] == \"categorical\":\n",
    "            remaining_data_left = np.delete(best_split[\"left\"], best_split[\"feature\"], axis=1)\n",
    "            remaining_data_right = np.delete(best_split[\"right\"], best_split[\"feature\"], axis=1)\n",
    "        else:\n",
    "            remaining_data_left = best_split[\"left\"]\n",
    "            remaining_data_right = best_split[\"right\"]\n",
    "        # Recursion\n",
    "        left_tree = self._build_tree(remaining_data_left, depth + 1)\n",
    "        right_tree = self._build_tree(remaining_data_right, depth + 1)\n",
    "        return Node(\n",
    "            left=left_tree,\n",
    "            right=right_tree,\n",
    "            feature=best_split[\"feature\"],\n",
    "            threshold=best_split[\"threshold\"]\n",
    "        )\n",
    "        \n",
    "    def _find_best_split(self, data):\n",
    "        \"\"\"\n",
    "        Find the best feature (and threshold) to split the current data.\n",
    "        This method iterates over all features (excluding the target) in the dataset to find the best split that \n",
    "        minimizes the weighted Gini impurity. It handles both continuous and categorical features:\n",
    "        - For continuous features, it finds the best threshold to split the data.\n",
    "            - Thresholds are the midpoints between unique values.\n",
    "        - For categorical features, binary splits are evaluated.\n",
    "\n",
    "        Parameters:\n",
    "        - data: 2D numpy array, the entire training dataset at the current node.\n",
    "        \n",
    "        Returns:\n",
    "        - A dictionary containing the best split information. It can be None if no valid split is found.\n",
    "        \"\"\"\n",
    "        best_gini = float(\"inf\") # use positive infinity since we will minimize it\n",
    "        best_split = None\n",
    "        n_features = data.shape[1] - 1 # last column is the target\n",
    "        for feature in range(n_features):\n",
    "            unique_values = np.unique(data[:, feature])\n",
    "            sorted_values = np.sort(unique_values)\n",
    "            if len(unique_values) > 2:\n",
    "                thresholds = (sorted_values[1:] + sorted_values[:-1]) / 2 # midpoints\n",
    "                for threshold in thresholds:\n",
    "                    left, right = self._split_continuous(data, feature, threshold)\n",
    "                    if len(left) == 0 or len(right) == 0:\n",
    "                        continue\n",
    "                    gini = self._gini_for_split(data, left, right)\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_split = {\n",
    "                            \"feature\": feature,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"left\": left,\n",
    "                            \"right\": right,\n",
    "                            \"type\": \"continuous\"\n",
    "                        }\n",
    "            else:\n",
    "                left, right = self._split_categorical(data, feature)\n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "                gini = self._gini_for_split(data, left, right)\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {\n",
    "                        \"feature\": feature,\n",
    "                        \"threshold\": None, # binary split\n",
    "                        \"left\": left,\n",
    "                        \"right\": right,\n",
    "                        \"type\": \"categorical\"\n",
    "                    }\n",
    "        return best_split\n",
    "    \n",
    "    # TODO: add a line to handle cases when len(data) = 0 in Gini?\n",
    "\n",
    "    def _gini_for_node(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity for a node.\n",
    "        \"\"\"\n",
    "        labels = data[:, -1] # the last column\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        probs = counts / len(data) # two probabilities of being in two classes\n",
    "        gini = 1 - np.sum(probs ** 2)\n",
    "        return gini\n",
    "\n",
    "    def _gini_for_split(self, data, left, right):\n",
    "        \"\"\"\n",
    "        Calculate the weighted Gini impurity for a split.\n",
    "        \"\"\"\n",
    "        total_size = len(data)\n",
    "        left_size = len(left)\n",
    "        right_size = len(right)\n",
    "        gini_left = self._gini_for_node(left)\n",
    "        gini_right = self._gini_for_node(right)\n",
    "        gini = (left_size / total_size) * gini_left + (right_size / total_size) * gini_right\n",
    "        return gini\n",
    "\n",
    "    def _split_continuous(self, data, feature_index, threshold):\n",
    "        \"\"\"\n",
    "        Split the data based on a continuous feature.\n",
    "\n",
    "        Parameters:\n",
    "        - data: 2D numpy array, the entire training dataset at the current node.\n",
    "        - feature_index: int, index of the feature to split.\n",
    "        \n",
    "        Returns:\n",
    "        - Two subsets of the data.\n",
    "        \"\"\"\n",
    "        left = data[data[:, feature_index] <= threshold]\n",
    "        right = data[data[:, feature_index] > threshold]\n",
    "        return left, right\n",
    "\n",
    "    def _split_categorical(self, data, feature_index):\n",
    "        \"\"\"\n",
    "        Split the data based on a categorical feature.\n",
    "\n",
    "        Parameters:\n",
    "        - data: 2D numpy array, the entire training dataset at the current node.\n",
    "        - feature_index: int, index of the feature to split.\n",
    "\n",
    "        Returns:\n",
    "        - Two subsets of the data.\n",
    "        \"\"\"\n",
    "        values = np.unique(data[:, feature_index])\n",
    "        threshold = np.mean(values)\n",
    "        left = data[data[:, feature_index] <= threshold]\n",
    "        right = data[data[:, feature_index] > threshold]\n",
    "        return left, right\n",
    "\n",
    "    def _predict_row(self, node, row):\n",
    "        \"\"\"\n",
    "        Predict the label for a single row. Traverse the tree to determine the predicted label.\n",
    "\n",
    "        Parameters:\n",
    "        - node: Node, the current node in the tree.\n",
    "        - row: 1D numpy array, a single data point.\n",
    "\n",
    "        Returns:\n",
    "        - The predicted label.\n",
    "        \"\"\"\n",
    "        if node.is_leaf():\n",
    "            return node.label\n",
    "        # Categorical split\n",
    "        if node.threshold is None:\n",
    "            return self._predict_row(node.left, row) if row[node.feature] == 0 else self._predict_row(node.right, row)\n",
    "        # Continuous split\n",
    "        else:  \n",
    "            return self._predict_row(node.left, row) if row[node.feature] <= node.threshold else self._predict_row(node.right, row)\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"\n",
    "        Predict the labels for the test data. Traverse the tree to determine the predicted labels.\n",
    "\n",
    "        Parameters:\n",
    "        - test_data: 2D numpy array, the test dataset.\n",
    "\n",
    "        Returns:\n",
    "        - A 1D numpy array containing the predicted labels\n",
    "        \"\"\"\n",
    "        if len(test_data.shape) == 0:\n",
    "            return np.array([]) # return an empty array\n",
    "        return np.array([self._predict_row(self.tree, row) for row in test_data])\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the loss for the data.\n",
    "        \"\"\"\n",
    "        preds = self.predict(data[:, :-1])\n",
    "        true_labels = data[:, -1]\n",
    "        return np.sum(preds != true_labels) / len(true_labels)\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy for the data.\n",
    "        \"\"\"\n",
    "        return 1 - self.loss(data)\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Visualize the decision tree.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            print(\"Empty tree.\")\n",
    "        else:\n",
    "            print(\"--- START PRINT TREE ---\")\n",
    "            self._visualize_tree(self.tree)\n",
    "            print(\"--- END PRINT TREE ---\")\n",
    "\n",
    "    def _visualize_tree(self, node, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively visualize the decision tree.\n",
    "        \"\"\"\n",
    "        indent = \"  \" * depth\n",
    "        if node.is_leaf():\n",
    "            print(f\"{indent}Predict -> {node.label}\")\n",
    "        else:\n",
    "            if node.threshold is None:\n",
    "                print(f\"{indent}Split attribute = {node.feature}; categorical\")\n",
    "            else:\n",
    "                print(f\"{indent}Split attribute = {node.feature}; threshold = {node.threshold:.3f}\")\n",
    "            print(f\"{indent}Left:\")\n",
    "            self._visualize_tree(node.left, depth + 1)\n",
    "            print(f\"{indent}Right:\")\n",
    "            self._visualize_tree(node.right, depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests for splitting\n",
    "def test_splitting():\n",
    "    # Check if the splitted sizes are correct\n",
    "    data = np.array([[i] for i in range(101)])\n",
    "    train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "    assert train_data.shape[0] == 71, \"Train set size is incorrect.\"\n",
    "    assert test_data.shape[0] == 30, \"Test set size is incorrect.\"\n",
    "    # Check if the function works well for empty data\n",
    "    data = np.array([]).reshape(0, 2)\n",
    "    train_data, test_data = train_test_split(data, test_size=0.4, random_state=42)\n",
    "    assert train_data.shape[0] == 0, \"Train set should be empty.\"\n",
    "    assert test_data.shape[0] == 0, \"Test set should be empty.\"\n",
    "    print(\"Splitting tests passed.\")\n",
    "\n",
    "# Tests for Node\n",
    "def test_Node():\n",
    "    # Check if the node is a leaf node\n",
    "    leaf_node = Node(label=1)\n",
    "    assert leaf_node.is_leaf(), \"Leaf node should be a leaf.\"\n",
    "    # Check if the node is a decision node\n",
    "    decision_node = Node(left=\"LeftNode\", right=\"RightNode\", feature=2, threshold=0.5)\n",
    "    assert not decision_node.is_leaf(), \"Decision node should not be a leaf.\"\n",
    "    print(\"Node tests passed.\")\n",
    "\n",
    "# Tests for _find_best_split\n",
    "def test_find_best_split():\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [2.0, 3.5, 1],\n",
    "        [1.5, 2.0, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_split = cart._find_best_split(data)\n",
    "    # Check a valid split is found and the split is correct\n",
    "    # Should not split on the target but split on one of the continuous features\n",
    "    assert best_split is not None, \"Best split should not be None.\"\n",
    "    assert best_split[\"type\"] == \"continuous\", \"Best split should be continuous.\"\n",
    "    assert best_split[\"feature\"] != 2, \"Best split should not be on the target.\"\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [1.0, 2.5, 0],\n",
    "        [1.0, 2.5, 0]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    best_split = cart._find_best_split(data)\n",
    "    # Check that no split should be found if all features are the same\n",
    "    assert best_split is None, \"Best split should be None.\"\n",
    "    print(\"Find best split tests passed.\")\n",
    "\n",
    "# Tests for Gini impurity calculations\n",
    "def test_gini():\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [2.0, 3.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    gini = cart._gini_for_node(data) # Expected = 0.5\n",
    "    assert abs(gini - 0.5) < 1e-6, \"Gini impurity for node is incorrect.\"\n",
    "    left = data[:1]\n",
    "    right = data[1:]\n",
    "    gini = cart._gini_for_split(data, left, right) # Expected = 0\n",
    "    assert abs(gini - 0) < 1e-6, \"Gini impurity for split is incorrect.\"\n",
    "    print(\"Gini tests passed.\")\n",
    "\n",
    "# Tests for splitting continuous features and categorical features\n",
    "def test_split_features():\n",
    "    data = np.array([\n",
    "        [1.0, 2.5, 0],\n",
    "        [2.0, 3.5, 1],\n",
    "        [1.0, 2.0, 0],\n",
    "        [2.0, 4.5, 1]\n",
    "    ])\n",
    "    cart = CART()\n",
    "    left, right = cart._split_continuous(data, 1, 3.0)\n",
    "    # Check if the split is correct for a continuous feature\n",
    "    assert len(left) == 2, \"Left split size is incorrect.\"\n",
    "    assert len(right) == 2, \"Right split size is incorrect.\"\n",
    "    left, right = cart._split_categorical(data, 2)\n",
    "    # Check if the split is correct for a categorical feature\n",
    "    assert len(left) == 2, \"Left split size is incorrect.\"\n",
    "    assert len(right) == 2, \"Right split size is incorrect.\"\n",
    "    print(\"Split features tests passed.\")\n",
    "\n",
    "# Tests for fit and _build_tree\n",
    "def test_fit():\n",
    "    data = np.array([\n",
    "        [1, 2, 0],\n",
    "        [3, 4, 1],\n",
    "        [1, 2, 0],\n",
    "        [3, 4, 1]\n",
    "    ])\n",
    "    cart = CART(max_depth=2, min_samples_split=2)\n",
    "    cart.fit(data)\n",
    "    # Check if the tree is built\n",
    "    assert cart.tree is not None, \"Tree should not be None after fitting.\"\n",
    "    assert cart.tree.feature is not None, \"Tree root should have a splitting feature.\"\n",
    "    data = np.empty((0, 3))\n",
    "    cart.fit(data)\n",
    "    # Check if the label is 0\n",
    "    assert cart.tree.label == 0, \"Incorrect label for empty data.\"\n",
    "    print(\"Fit tests passed.\")\n",
    "\n",
    "# Tests for predict\n",
    "def test_predict():\n",
    "    # Check if a single row prediction is correct\n",
    "    tree = Node(left=Node(label=1), right=Node(label=0), feature=0, threshold=1.5)\n",
    "    cart = CART()\n",
    "    row = np.array([1.0, 2.0])  # Expected: left -> 1\n",
    "    pred = cart._predict_row(tree, row)\n",
    "    assert pred == 1, \"Prediction for single row is incorrect.\"\n",
    "    # Check predictions for a dataset\n",
    "    cart.tree = tree\n",
    "    data = np.array([\n",
    "        [1.0, 2.0],\n",
    "        [2.0, 3.0]\n",
    "    ])\n",
    "    preds = cart.predict(data)\n",
    "    assert np.array_equal(preds, [1, 0]), \"Batch predictions are incorrect.\"\n",
    "    print(\"Predict tests passed.\")\n",
    "\n",
    "# Tests for loss and accuracy\n",
    "def test_loss_acc():\n",
    "    tree = Node(left=Node(label=1), right=Node(label=0), feature=0, threshold=1.5)\n",
    "    cart = CART()\n",
    "    cart.tree = tree\n",
    "    data = np.array([\n",
    "        [1.0, 2.0, 1],\n",
    "        [2.0, 3.0, 0],\n",
    "        [0.5, 1.0, 1],\n",
    "        [3.0, 4.0, 0]\n",
    "    ])\n",
    "    # Check if loss = 0 and accuracy = 1\n",
    "    assert cart.loss(data) == 0.0, \"Loss calculation is incorrect.\"\n",
    "    assert cart.accuracy(data) == 1.0, \"Accuracy calculation is incorrect.\"\n",
    "    print(\"Loss and accuracy tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting tests passed.\n",
      "Node tests passed.\n",
      "Find best split tests passed.\n",
      "Gini tests passed.\n",
      "Split features tests passed.\n",
      "Fit tests passed.\n",
      "Predict tests passed.\n",
      "Loss and accuracy tests passed.\n"
     ]
    }
   ],
   "source": [
    "test_splitting()\n",
    "test_Node()\n",
    "test_find_best_split()\n",
    "test_gini()\n",
    "test_split_features()\n",
    "test_fit()\n",
    "test_predict()\n",
    "test_loss_acc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.934\n",
      "Testing accuracy: 0.793\n",
      "--- START PRINT TREE ---\n",
      "Split attribute = 11; threshold = 0.500\n",
      "Left:\n",
      "  Split attribute = 12; threshold = 2.500\n",
      "  Left:\n",
      "    Split attribute = 9; threshold = 2.700\n",
      "    Left:\n",
      "      Split attribute = 7; threshold = 92.500\n",
      "      Left:\n",
      "        Predict -> 0.0\n",
      "      Right:\n",
      "        Split attribute = 3; threshold = 158.000\n",
      "        Left:\n",
      "          Split attribute = 9; threshold = 1.700\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 1\n",
      "        Right:\n",
      "          Predict -> 1\n",
      "    Right:\n",
      "      Predict -> 0\n",
      "  Right:\n",
      "    Split attribute = 7; threshold = 143.500\n",
      "    Left:\n",
      "      Split attribute = 9; threshold = 0.250\n",
      "      Left:\n",
      "        Predict -> 1.0\n",
      "      Right:\n",
      "        Predict -> 0.0\n",
      "    Right:\n",
      "      Split attribute = 2; threshold = 0.500\n",
      "      Left:\n",
      "        Predict -> 0\n",
      "      Right:\n",
      "        Split attribute = 0; threshold = 39.000\n",
      "        Left:\n",
      "          Predict -> 0.0\n",
      "        Right:\n",
      "          Split attribute = 0; threshold = 63.500\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0.0\n",
      "Right:\n",
      "  Split attribute = 2; threshold = 0.500\n",
      "  Left:\n",
      "    Split attribute = 7; threshold = 108.500\n",
      "    Left:\n",
      "      Predict -> 0\n",
      "    Right:\n",
      "      Predict -> 0.0\n",
      "  Right:\n",
      "    Split attribute = 10; threshold = 1.500\n",
      "    Left:\n",
      "      Split attribute = 7; threshold = 145.500\n",
      "      Left:\n",
      "        Predict -> 0.0\n",
      "      Right:\n",
      "        Predict -> 0\n",
      "    Right:\n",
      "      Split attribute = 3; threshold = 153.000\n",
      "      Left:\n",
      "        Split attribute = 9; threshold = 2.550\n",
      "        Left:\n",
      "          Split attribute = 4; threshold = 318.500\n",
      "          Left:\n",
      "            Predict -> 1.0\n",
      "          Right:\n",
      "            Predict -> 0\n",
      "        Right:\n",
      "          Predict -> 0.0\n",
      "      Right:\n",
      "        Predict -> 0.0\n",
      "--- END PRINT TREE ---\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"heart.csv\", delimiter=\",\", skiprows=1)\n",
    "train_data, test_data = train_test_split(data, test_size=0.4, random_state=2060)\n",
    "\n",
    "model = CART(max_depth=10, min_samples_split=10)\n",
    "model.fit(train_data)\n",
    "train_accuracy = model.accuracy(train_data)\n",
    "test_accuracy = model.accuracy(test_data)\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Testing accuracy: {test_accuracy:.3f}\")\n",
    "model.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the CART model using scikit-learning is written by our group members, Yixun Kang and David Ning. The model use a pipeline structure, including the preprocessor and the algorithm. In the preprocessor, we used One-Hot encoder for two categorical features, `sex` and `exange` and then applied `StandardScaler()` to all features. We used K-Fold as the cross validation and 6/2/2 for train/test/validation. In the parameter grid, we tuned for `min_samples_leaf` and `max_leaf_nodes` and keep the `max_depth` and `min_samples_split` the same as in main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart.csv\")\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor\n",
    "cat_ftrs = [\"sex\", \"exang\"]\n",
    "num_ftrs = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"cp\", \"fbs\", \"restecg\", \"slope\", \"ca\", \"thal\"]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", categorical_transformer, cat_ftrs),\n",
    "    (\"num\", numerical_transformer, num_ftrs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML pipeline\n",
    "def MLpipe_kfold(X, y, random_states, preprocessor, ML_algo, param_grid, n_splits=5):\n",
    "    test_scores = []\n",
    "    best_models = []\n",
    "    for i, random_state in enumerate(random_states):\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        pipe = make_pipeline(preprocessor, ML_algo)\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid, cv=kf, n_jobs=-1, return_train_score=True, \n",
    "                            verbose=True, scoring=\"accuracy\")\n",
    "        grid.fit(X_other, y_other)\n",
    "        results = pd.DataFrame(grid.cv_results_)\n",
    "        best_models.append(grid)\n",
    "        y_test_pred = best_models[-1].predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_scores.append(test_accuracy)\n",
    "    return test_scores, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n",
      "Average Testing Accuracy: 0.8360655737704918\n"
     ]
    }
   ],
   "source": [
    "random_states = [2060]\n",
    "ML_algo = DecisionTreeClassifier(random_state=2060, criterion=\"gini\", max_depth=10, min_samples_split=10)\n",
    "param_grid = {\n",
    "    \"decisiontreeclassifier__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"decisiontreeclassifier__max_leaf_nodes\": [5, 10]\n",
    "}\n",
    "test_scores, best_models = MLpipe_kfold(X, y, random_states, preprocessor, ML_algo, param_grid, n_splits=10)\n",
    "print(\"Average Testing Accuracy:\", np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn average testing accuracy is 83.6% and the implementation testing accuracy is 79.3%. The scikit-learn model perform slight better than the implementation. Also, in the implemented model, we observed some levels of overfitting. We will consider using cross-validation and pruning method later to optimize the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
